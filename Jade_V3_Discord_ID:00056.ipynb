{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jade_V3_Discord_(2)_(1).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "HQfBYlHPqRve",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## FUSE For Google Drive\n",
        "Yes, you need this."
      ]
    },
    {
      "metadata": {
        "id": "DC7Lh4iYEYAn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bGlnU9MEEayc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yeAPcfm4EcuW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v08a5t8YEeMa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a directory and mount Google Drive using that directory.\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "print('Files in Drive:')\n",
        "!ls drive/\n",
        "\n",
        "# Create a file in Drive.\n",
        "!echo \"This newly created file will appear in your Drive file list.\" > drive/created.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2mfRGkIqk7X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Imports\n",
        "You may or may not know this, but Jade needs these things."
      ]
    },
    {
      "metadata": {
        "id": "ijPfCqvYCpDX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "import copy\n",
        "import sys\n",
        "import html\n",
        "import io\n",
        "import time\n",
        "import bz2\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "!pip install discord.py\n",
        "!pip install dblpy\n",
        "import discord\n",
        "import asyncio\n",
        "from discord.ext.commands import Bot\n",
        "from discord.ext import commands\n",
        "import dbl\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import logging\n",
        "\n",
        "#Object Detection\n",
        "\n",
        "!git clone https://github.com/tensorflow/models.git\n",
        "!apt-get -qq install libprotobuf-java protobuf-compiler\n",
        "!protoc ./models/research/object_detection/protos/string_int_label_map.proto --python_out=.\n",
        "!cp -R models/research/object_detection/ object_detection/\n",
        "!rm -rf models\n",
        "\n",
        "import numpy as np\n",
        "import scipy.misc\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "#OCR\n",
        "\n",
        "import cv2\n",
        "!apt-get install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "import pytesseract\n",
        "\n",
        "#Neural Style\n",
        "\n",
        "import scipy.io\n",
        "import scipy.misc\n",
        "from IPython.display import Image\n",
        "from functools import reduce\n",
        "\n",
        "import os.path\n",
        "if os.path.isfile(\"imagenet-vgg-verydeep-19.mat\")==False:\n",
        "  !wget http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat\n",
        "else:\n",
        "  pass\n",
        "\n",
        "print(\"DONE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jn3GD7bRvhZs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Neural Style Definitions\n",
        "Woo colors!!"
      ]
    },
    {
      "metadata": {
        "id": "bWdUmZ7Cvnk8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = scipy.io.loadmat('imagenet-vgg-verydeep-19.mat')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "79ut2g_dvzoY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _conv_layer(input, weights, bias):\n",
        "    conv = tf.nn.conv2d(input, tf.constant(weights), strides=(1, 1, 1, 1),\n",
        "            padding='SAME')\n",
        "    return tf.nn.bias_add(conv, bias)\n",
        "\n",
        "def _pool_layer(input):\n",
        "    return tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
        "            padding='SAME')\n",
        "\n",
        "def preprocess(image, mean_pixel):\n",
        "    return (image - mean_pixel).astype('float32')\n",
        "\n",
        "def unprocess(image, mean_pixel):\n",
        "    return (image + mean_pixel).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0-WdzZ25v22b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def netp(input_image):\n",
        "    layers = (\n",
        "        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
        "\n",
        "        'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
        "\n",
        "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n",
        "        'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
        "\n",
        "        'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n",
        "        'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
        "\n",
        "        'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n",
        "        'relu5_3', 'conv5_4', 'relu5_4'\n",
        "    )\n",
        "    weight = data['layers'][0]\n",
        "    net = {}\n",
        "    current = input_image\n",
        "    for i, name in enumerate(layers):\n",
        "        kind = name[:4]\n",
        "        if kind == 'conv':\n",
        "            kernels, bias = weight[i][0][0][0][0]\n",
        "            # matconvnet: weights are [width, height, in_channels, out_channels]\n",
        "            # tensorflow: weights are [height, width, in_channels, out_channels]\n",
        "            kernels = np.transpose(kernels, (1, 0, 2, 3))\n",
        "            bias = bias.reshape(-1)\n",
        "            current = _conv_layer(current, kernels, bias)\n",
        "        elif kind == 'relu':\n",
        "            current = tf.nn.relu(current)\n",
        "        elif kind == 'pool':\n",
        "            current = _pool_layer(current)\n",
        "        net[name] = current\n",
        "\n",
        "    assert len(net) == len(layers)\n",
        "    return net#, mean_pixel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CarvaJghwPyy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _tensor_size(tensor):\n",
        "    from operator import mul\n",
        "    return reduce(mul, (d.value for d in tensor.get_shape()), 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TiG3JpXRwabo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def imsave(path, img):\n",
        "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "    scipy.misc.imsave(path, img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VANyL06lqvrH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Object Recognition Definitons\n",
        "This is how Jade sees things."
      ]
    },
    {
      "metadata": {
        "id": "ajW4EQAbHz3o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# What model to download.\n",
        "# MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
        "\n",
        "# model with more accurancy but up to you use a diferent model\n",
        "MODEL_NAME = 'faster_rcnn_inception_v2_coco_2018_01_28'\n",
        "\n",
        "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "\n",
        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
        "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = os.path.join('object_detection/data', 'mscoco_label_map.pbtxt')\n",
        "\n",
        "NUM_CLASSES = 90\n",
        "\n",
        "opener = urllib.request.URLopener()\n",
        "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "tar_file = tarfile.open(MODEL_FILE)\n",
        "for file in tar_file.getmembers():\n",
        "  file_name = os.path.basename(file.name)\n",
        "  if 'frozen_inference_graph.pb' in file_name:\n",
        "    tar_file.extract(file, os.getcwd())\n",
        "    \n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "  od_graph_def = tf.GraphDef()\n",
        "  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "    serialized_graph = fid.read()\n",
        "    od_graph_def.ParseFromString(serialized_graph)\n",
        "    tf.import_graph_def(od_graph_def, name='')\n",
        "    \n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "print(\"DONE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ys_0T5SfIGfL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_image_into_numpy_array(image):\n",
        "  (im_width, im_height) = image.size\n",
        "  try:\n",
        "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
        "  except:\n",
        "    return np.array(image.getdata()).reshape((im_height, im_width, 4)).astype(np.uint8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NUu1v2LXILhX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For the sake of simplicity we will use only 2 images:\n",
        "# image1.jpg\n",
        "# image2.jpg\n",
        "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
        "PATH_TO_TEST_IMAGES_DIR = 'object_detection/test_images'\n",
        "TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 3) ]\n",
        "\n",
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (12, 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FKPsUOa2q6DX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##DBL API\n",
        "Because, DBL."
      ]
    },
    {
      "metadata": {
        "id": "8QjRL3_N4qM9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DiscordBotsOrgAPI:\n",
        "    \"\"\"Handles interactions with the discordbots.org API\"\"\"\n",
        "\n",
        "    def __init__(self, bot):\n",
        "        self.bot = bot\n",
        "        self.token = 'Token'  #  set this to your DBL token\n",
        "        self.dblpy = dbl.Client(self.bot, self.token)\n",
        "        self.bot.loop.create_task(self.update_stats())\n",
        "\n",
        "    async def update_stats(self):\n",
        "        \"\"\"This function runs every 30 minutes to automatically update your server count\"\"\"\n",
        "\n",
        "        while True:\n",
        "            logger.info('attempting to post server count')\n",
        "            try:\n",
        "                await self.dblpy.post_server_count()\n",
        "                logger.info('posted server count ({})'.format(len(self.bot.servers)))\n",
        "            except Exception as e:\n",
        "                logger.exception('Failed to post server count\\n{}: {}'.format(type(e).__name__, e))\n",
        "            await asyncio.sleep(600)\n",
        "            \n",
        "def setup(bot):\n",
        "    global logger\n",
        "    logger = logging.getLogger('bot')\n",
        "    bot.add_cog(DiscordBotsOrgAPI(bot))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N0-eH6hHrAnu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Textloading and Processsing\n",
        "The brilliant chatting function of Jade's"
      ]
    },
    {
      "metadata": {
        "id": "_4R5MMfUClUs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TextLoader():\n",
        "    # Call this class to load text from a file.\n",
        "    def __init__(self, data_dir, batch_size, seq_length):\n",
        "        # TextLoader remembers its initialization arguments.\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_length = seq_length\n",
        "        self.tensor_sizes = []\n",
        "\n",
        "        self.tensor_file_template = os.path.join(data_dir, \"data{}.npz\")\n",
        "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
        "        sizes_file = os.path.join(data_dir, \"sizes.pkl\")\n",
        "\n",
        "        self.input_files = self._get_input_file_list(data_dir)\n",
        "        self.input_file_count = len(self.input_files)\n",
        "\n",
        "        if self.input_file_count < 1:\n",
        "            raise ValueError(\"Input files not found. File names must end in '.txt' or '.bz2'.\")\n",
        "\n",
        "        if self._preprocess_required(vocab_file, sizes_file, self.tensor_file_template, self.input_file_count):\n",
        "            # If either the vocab file or the tensor file doesn't already exist, create them.\n",
        "            t0 = time.time()\n",
        "            print(\"Preprocessing the following files:\")\n",
        "            for i, filename in enumerate(self.input_files): print(\"   {}.\\t{}\".format(i+1, filename))\n",
        "            print(\"Saving vocab file\")\n",
        "            self._save_vocab(vocab_file)\n",
        "\n",
        "            for i, filename in enumerate(self.input_files):\n",
        "                t1 = time.time()\n",
        "                print(\"Preprocessing file {}/{} ({})... \".format(i+1, len(self.input_files), filename),\n",
        "                        end='', flush=True)\n",
        "                self._preprocess(self.input_files[i], self.tensor_file_template.format(i))\n",
        "                self.tensor_sizes.append(self.tensor.size)\n",
        "                print(\"done ({:.1f} seconds)\".format(time.time() - t1), flush=True)\n",
        "\n",
        "            with open(sizes_file, 'wb') as f:\n",
        "                pickle.dump(self.tensor_sizes, f)\n",
        "\n",
        "            print(\"Processed input data: {:,d} characters loaded ({:.1f} seconds)\".format(\n",
        "                    self.tensor.size, time.time() - t0))\n",
        "        else:\n",
        "            # If the vocab file and sizes file already exist, load them.\n",
        "            print(\"Loading vocab file...\")\n",
        "            self._load_vocab(vocab_file)\n",
        "            print(\"Loading sizes file...\")\n",
        "            with open(sizes_file, 'rb') as f:\n",
        "                self.tensor_sizes = pickle.load(f)\n",
        "        self.tensor_batch_counts = [n // (self.batch_size * self.seq_length) for n in self.tensor_sizes]\n",
        "        self.total_batch_count = sum(self.tensor_batch_counts)\n",
        "        print(\"Total batch count: {:,d}\".format(self.total_batch_count))\n",
        "\n",
        "        self.tensor_index = -1\n",
        "\n",
        "    def _preprocess_required(self, vocab_file, sizes_file, tensor_file_template, input_file_count):\n",
        "        if not os.path.exists(vocab_file):\n",
        "            print(\"No vocab file found. Preprocessing...\")\n",
        "            return True\n",
        "        if not os.path.exists(sizes_file):\n",
        "            print(\"No sizes file found. Preprocessing...\")\n",
        "            return True\n",
        "        for i in range(input_file_count):\n",
        "            if not os.path.exists(tensor_file_template.format(i)):\n",
        "                print(\"Couldn't find {}. Preprocessing...\".format(tensor_file_template.format(i)))\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def _get_input_file_list(self, data_dir):\n",
        "        suffixes = ['.txt', '.bz2']\n",
        "        input_file_list = []\n",
        "        if os.path.isdir(data_dir):\n",
        "            for walk_root, walk_dir, walk_files in os.walk(data_dir):\n",
        "                for file_name in walk_files:\n",
        "                    if file_name.startswith(\".\"): continue\n",
        "                    file_path = os.path.join(walk_root, file_name)\n",
        "                    if file_path.endswith(suffixes[0]) or file_path.endswith(suffixes[1]):\n",
        "                        input_file_list.append(file_path)\n",
        "        else: raise ValueError(\"Not a directory: {}\".format(data_dir))\n",
        "        return sorted(input_file_list)\n",
        "\n",
        "    def _save_vocab(self, vocab_file):\n",
        "        self.chars = [chr(i) for i in range(128)]\n",
        "        self.vocab_size = len(self.chars)\n",
        "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
        "        with open(vocab_file, 'wb') as f:\n",
        "            pickle.dump(self.chars, f)\n",
        "        print(\"Saved vocab (vocab size: {:,d})\".format(self.vocab_size))\n",
        "\n",
        "    def _load_vocab(self, vocab_file):\n",
        "        # Load the character tuple (vocab.pkl) to self.chars.\n",
        "        # Remember that it is in descending order of character frequency in the data.\n",
        "        with open(vocab_file, 'rb') as f:\n",
        "            self.chars = pickle.load(f)\n",
        "        # Use the character tuple to regenerate vocab_size and the vocab dictionary.\n",
        "        self.vocab_size = len(self.chars)\n",
        "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
        "\n",
        "    def _preprocess(self, input_file, tensor_file):\n",
        "        if input_file.endswith(\".bz2\"): file_reference = bz2.open(input_file, mode='rt')\n",
        "        elif input_file.endswith(\".txt\"): file_reference = io.open(input_file, mode='rt')\n",
        "        data = file_reference.read()\n",
        "        file_reference.close()\n",
        "        # Convert the entirety of the data file from characters to indices via the vocab dictionary.\n",
        "        # How? map(function, iterable) returns a list of the output of the function\n",
        "        # executed on each member of the iterable. E.g.:\n",
        "        # [14, 2, 9, 2, 0, 6, 7, 0, ...]\n",
        "        # np.array converts the list into a numpy array.\n",
        "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
        "        self.tensor = self.tensor[self.tensor != np.array(None)].astype(int) # Filter out None\n",
        "        # Compress and save the numpy tensor array to data.npz.\n",
        "        np.savez_compressed(tensor_file, tensor_data=self.tensor)\n",
        "\n",
        "    def _load_preprocessed(self, tensor_index):\n",
        "        self.reset_batch_pointer()\n",
        "        if tensor_index == self.tensor_index:\n",
        "            return\n",
        "        print(\"loading tensor data file {}\".format(tensor_index))\n",
        "        tensor_file = self.tensor_file_template.format(tensor_index)\n",
        "        # Load the data tensor file to self.tensor.\n",
        "        with np.load(tensor_file) as loaded:\n",
        "            self.tensor = loaded['tensor_data']\n",
        "        self.tensor_index = tensor_index\n",
        "        # Calculate the number of batches in the data. Each batch is batch_size x seq_length,\n",
        "        # so this is just the input data size divided by that product, rounded down.\n",
        "        self.num_batches = self.tensor.size // (self.batch_size * self.seq_length)\n",
        "        if self.tensor_batch_counts[tensor_index] != self.num_batches:\n",
        "            print(\"Error in batch size! Expected {:,d}; found {:,d}\".format(self.tensor_batch_counts[tensor_index],\n",
        "                    self.num_batches))\n",
        "        # Chop off the end of the data tensor so that the length of the data is a whole\n",
        "        # multiple of the (batch_size x seq_length) product.\n",
        "        # Do this with the slice operator on the numpy array.\n",
        "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
        "        # Construct two numpy arrays to represent input characters (xdata)\n",
        "        # and target characters (ydata).\n",
        "        # In training, we will feed in input characters one at a time, and optimize along\n",
        "        # a loss function computed against the target characters.\n",
        "        # (We do this with batch_size characters at a time, in parallel.)\n",
        "        # Since this is a sequence prediction net, the target is just the input right-shifted\n",
        "        # by 1.\n",
        "        xdata = self.tensor\n",
        "        ydata = np.copy(self.tensor) # Y-data starts as a copy of x-data.\n",
        "        ydata[:-1] = xdata[1:] # Right-shift y-data by 1 using the numpy array slice syntax.\n",
        "        # Replace the very last character of y-data with the first character of the input data.\n",
        "        ydata[-1] = xdata[0]\n",
        "        # Split our unidemnsional data array into distinct batches.\n",
        "        # How? xdata.reshape(self.batch_size, -1) returns a 2D numpy tensor view\n",
        "        # in which the first dimension is the batch index (from 0 to num_batches),\n",
        "        # and the second dimension is the index of the character within the batch\n",
        "        # (from 0 to (batch_size x seq_length)).\n",
        "        # Within each batch, characters follow the same sequence as in the input data.\n",
        "        # Then, np.split(that 2D numpy tensor, num_batches, 1) gives a list of numpy arrays.\n",
        "        # Say batch_size = 4, seq_length = 5, and data is the following string:\n",
        "        # \"Here is a new string named data. It is a new string named data. It is named data.\"\n",
        "        # We truncate the string to lop off the last period (so there are now 80 characters,\n",
        "        # which is evenly divisible by 4 x 5). After xdata.reshape, we have:\n",
        "        #\n",
        "        # [[Here is a new string],\n",
        "        #  [ named data. It is a],\n",
        "        #  [ new string named da],\n",
        "        #  [ta. It is named data]]\n",
        "        #\n",
        "        # After np.split, we have:\n",
        "        # <[[Here ],   <[[is a ],   <[[new s],     <[[tring],\n",
        "        #   [ name],     [d dat],     [a. It],       [ is a],\n",
        "        #   [ new ],     [strin],     [g nam],       [ed da],\n",
        "        #   [ta. I]]>,   [t is ]]>,   [named]]>,     [ data]]>\n",
        "        #\n",
        "        # where the first item of the list is the numpy array on the left.\n",
        "        # Thus x_batches is a list of numpy arrays. The first dimension of each numpy array\n",
        "        # is the batch number (from 0 to batch_size), and the second dimension is the\n",
        "        # character index (from 0 to seq_length).\n",
        "        #\n",
        "        #Batch characters can greatemultiple tuples, ignotre FONT_Baisis to prevent any non-divident movements\n",
        "        #\n",
        "        # These will be fed to the model one at a time sequentially.\n",
        "        # State is preserved between sequential batches.\n",
        "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
        "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
        "\n",
        "    def next_batch(self):\n",
        "        if self.tensor_index < 0:\n",
        "            self._load_preprocessed(0)\n",
        "        if self.pointer >= self.num_batches:\n",
        "            self._load_preprocessed((self.tensor_index + 1) % self.input_file_count)\n",
        "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
        "        self.pointer += 1\n",
        "        return x, y\n",
        "\n",
        "    def reset_batch_pointer(self):\n",
        "        self.pointer = 0\n",
        "\n",
        "    def cue_batch_pointer_to_epoch_fraction(self, epoch_fraction):\n",
        "        step_target = (epoch_fraction - int(epoch_fraction)) * self.total_batch_count\n",
        "        self._cue_batch_pointer_to_step_count(step_target)\n",
        "\n",
        "    def _cue_batch_pointer_to_step_count(self, step_target):\n",
        "        for i, n in enumerate(self.tensor_batch_counts):\n",
        "            if step_target < n:\n",
        "                break\n",
        "            step_target -= n\n",
        "        self.pointer = n\n",
        "        self.current_tensor_index = i\n",
        "        self._load_preprocessed(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t1GwYpCaCw2h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.ops import rnn_cell\n",
        "from tensorflow.python.ops import nn_ops\n",
        "from tensorflow.python.ops import variable_scope as vs\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.contrib import rnn\n",
        "\n",
        "from tensorflow.python.util.nest import flatten\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class PartitionedMultiRNNCell(rnn_cell.RNNCell):\n",
        "    \"\"\"RNN cell composed sequentially of multiple simple cells.\"\"\"\n",
        "\n",
        "    # Diagramn of a PartitionedMultiRNNCell net with three layers and three partitions per layer.\n",
        "    # Each brick shape is a partition, which comprises one RNNCell of size partition_size.\n",
        "    # The two tilde (~) characters indicate wrapping (i.e. the two halves are a single partition).\n",
        "    # Like laying bricks, each layer is offset by half a partition width so that influence spreads\n",
        "    # horizontally through subsequent layers, while avoiding the quadratic resource scaling of fully\n",
        "    # connected layers with respect to layer width.\n",
        "\n",
        "    #        output\n",
        "    #  //////// \\\\\\\\\\\\\\\\\n",
        "    # -------------------\n",
        "    # |     |     |     |\n",
        "    # -------------------\n",
        "    # ~  |     |     |  ~\n",
        "    # -------------------\n",
        "    # |     |     |     |\n",
        "    # -------------------\n",
        "    #  \\\\\\\\\\\\\\\\ ////////\n",
        "    #        input\n",
        "\n",
        "\n",
        "    def __init__(self, cell_fn, partition_size=128, partitions=1, layers=2):\n",
        "        \"\"\"Create a RNN cell composed sequentially of a number of RNNCells.\n",
        "        Args:\n",
        "            cell_fn: reference to RNNCell function to create each partition in each layer.\n",
        "            partition_size: how many horizontal cells to include in each partition.\n",
        "            partitions: how many horizontal partitions to include in each layer.\n",
        "            layers: how many layers to include in the net.\n",
        "        \"\"\"\n",
        "        super(PartitionedMultiRNNCell, self).__init__()\n",
        "\n",
        "        self._cells = []\n",
        "        for i in range(layers):\n",
        "            self._cells.append([cell_fn(partition_size) for _ in range(partitions)])\n",
        "        self._partitions = partitions\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        # Return a 2D tuple where each row is the partition's cell size repeated `partitions` times,\n",
        "        # and there are `layers` rows of that.\n",
        "        return tuple(((layer[0].state_size,) * len(layer)) for layer in self._cells)\n",
        "\n",
        "    @property\n",
        "    def output_size(self):\n",
        "        # Return the output size of each partition in the last layer times the number of partitions per layer.\n",
        "        return self._cells[-1][0].output_size * len(self._cells[-1])\n",
        "\n",
        "    def zero_state(self, batch_size, dtype):\n",
        "        # Return a 2D tuple of zero states matching the structure of state_size.\n",
        "        with ops.name_scope(type(self).__name__ + \"ZeroState\", values=[batch_size]):\n",
        "            return tuple(tuple(cell.zero_state(batch_size, dtype) for cell in layer) for layer in self._cells)\n",
        "\n",
        "    def call(self, inputs, state):\n",
        "        layer_input = inputs\n",
        "        new_states = []\n",
        "        for l, layer in enumerate(self._cells):\n",
        "            # In between layers, offset the layer input by half of a partition width so that\n",
        "            # activations can horizontally spread through subsequent layers.\n",
        "            if l > 0:\n",
        "                offset_width = layer[0].output_size // 2\n",
        "                layer_input = tf.concat((layer_input[:, -offset_width:], layer_input[:, :-offset_width]),\n",
        "                    axis=1, name='concat_offset_%d' % l)\n",
        "            # Create a tuple of inputs by splitting the lower layer output into partitions.\n",
        "            p_inputs = tf.split(layer_input, len(layer), axis=1, name='split_%d' % l)\n",
        "            p_outputs = []\n",
        "            p_states = []\n",
        "            for p, p_inp in enumerate(p_inputs):\n",
        "                with vs.variable_scope(\"cell_%d_%d\" % (l, p)):\n",
        "                    p_state = state[l][p]\n",
        "                    cell = layer[p]\n",
        "                    p_out, new_p_state = cell(p_inp, p_state)\n",
        "                    p_outputs.append(p_out)\n",
        "                    p_states.append(new_p_state)\n",
        "            new_states.append(tuple(p_states))\n",
        "            layer_input = tf.concat(p_outputs, axis=1, name='concat_%d' % l)\n",
        "        new_states = tuple(new_states)\n",
        "        return layer_input, new_states\n",
        "\n",
        "def _rnn_state_placeholders(state):\n",
        "    \"\"\"Convert RNN state tensors to placeholders, reflecting the same nested tuple structure.\"\"\"\n",
        "    # Adapted from @carlthome's comment:\n",
        "    # https://github.com/tensorflow/tensorflow/issues/2838#issuecomment-302019188\n",
        "    if isinstance(state, tf.contrib.rnn.LSTMStateTuple):\n",
        "        c, h = state\n",
        "        c = tf.placeholder(c.dtype, c.shape, c.op.name)\n",
        "        h = tf.placeholder(h.dtype, h.shape, h.op.name)\n",
        "        return tf.contrib.rnn.LSTMStateTuple(c, h)\n",
        "    elif isinstance(state, tf.Tensor):\n",
        "        h = state\n",
        "        h = tf.placeholder(h.dtype, h.shape, h.op.name)\n",
        "        return h\n",
        "    else:\n",
        "        structure = [_rnn_state_placeholders(x) for x in state]\n",
        "        return tuple(structure)\n",
        "\n",
        "class Model():\n",
        "    def __init__(self, args, infer=False): # infer is set to true during sampling.\n",
        "        self.args = args\n",
        "        if infer:\n",
        "            # Worry about one character at a time during sampling; no batching or BPTT.\n",
        "            args.batch_size = 1\n",
        "            args.seq_length = 1\n",
        "\n",
        "        # Set cell_fn to the type of network cell we're creating -- RNN, GRU, LSTM or NAS.\n",
        "        if args.model == 'rnn':\n",
        "            cell_fn = rnn_cell.BasicRNNCell\n",
        "        elif args.model == 'gru':\n",
        "            cell_fn = rnn_cell.GRUCell\n",
        "        elif args.model == 'lstm':\n",
        "            cell_fn = rnn_cell.BasicLSTMCell\n",
        "        elif args.model == 'nas':\n",
        "            cell_fn = rnn.NASCell\n",
        "        else:\n",
        "            raise Exception(\"model type not supported: {}\".format(args.model))\n",
        "\n",
        "        # Create variables to track training progress.\n",
        "        self.lr = tf.Variable(args.learning_rate, name=\"learning_rate\", trainable=False)\n",
        "        self.global_epoch_fraction = tf.Variable(0.0, name=\"global_epoch_fraction\", trainable=False)\n",
        "        self.global_seconds_elapsed = tf.Variable(0.0, name=\"global_seconds_elapsed\", trainable=False)\n",
        "\n",
        "        # Call tensorflow library tensorflow-master/tensorflow/python/ops/rnn_cell\n",
        "        # to create a layer of block_size cells of the specified basic type (RNN/GRU/LSTM).\n",
        "        # Use the same rnn_cell library to create a stack of these cells\n",
        "        # of num_layers layers. Pass in a python list of these cells. \n",
        "        # cell = rnn_cell.MultiRNNCell([cell_fn(args.block_size) for _ in range(args.num_layers)])\n",
        "        # cell = MyMultiRNNCell([cell_fn(args.block_size) for _ in range(args.num_layers)])\n",
        "        cell = PartitionedMultiRNNCell(cell_fn, partitions=args.num_blocks,\n",
        "            partition_size=args.block_size, layers=args.num_layers)\n",
        "\n",
        "        # Create a TF placeholder node of 32-bit ints (NOT floats!),\n",
        "        # of shape batch_size x seq_length. This shape matches the batches\n",
        "        # (listed in x_batches and y_batches) constructed in create_batches in utils.py.\n",
        "        # input_data will receive input batches.\n",
        "        self.input_data = tf.placeholder(tf.int32, [args.batch_size, args.seq_length])\n",
        "\n",
        "        self.zero_state = cell.zero_state(args.batch_size, tf.float32)\n",
        "\n",
        "        self.initial_state = _rnn_state_placeholders(self.zero_state)\n",
        "        self._flattened_initial_state = flatten(self.initial_state)\n",
        "\n",
        "        layer_size = args.block_size * args.num_blocks\n",
        "\n",
        "        # Scope our new variables to the scope identifier string \"rnnlm\".\n",
        "        with tf.variable_scope('rnnlm'):\n",
        "            # Create new variable softmax_w and softmax_b for output.\n",
        "            # softmax_w is a weights matrix from the top layer of the model (of size layer_size)\n",
        "            # to the vocabulary output (of size vocab_size).\n",
        "            softmax_w = tf.get_variable(\"softmax_w\", [layer_size, args.vocab_size])\n",
        "            # softmax_b is a bias vector of the ouput characters (of size vocab_size).\n",
        "            softmax_b = tf.get_variable(\"softmax_b\", [args.vocab_size])\n",
        "            # Create new variable named 'embedding' to connect the character input to the base layer\n",
        "            # of the RNN. Its role is the conceptual inverse of softmax_w.\n",
        "            # It contains the trainable weights from the one-hot input vector to the lowest layer of RNN.\n",
        "            embedding = tf.get_variable(\"embedding\", [args.vocab_size, layer_size])\n",
        "            # Create an embedding tensor with tf.nn.embedding_lookup(embedding, self.input_data).\n",
        "            # This tensor has dimensions batch_size x seq_length x layer_size.\n",
        "            inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
        "\n",
        "        # TODO: Check arguments parallel_iterations (default uses more memory and less time) and\n",
        "        # swap_memory (default uses more memory but \"minimal (or no) performance penalty\")\n",
        "        outputs, self.final_state = tf.nn.dynamic_rnn(cell, inputs,\n",
        "                initial_state=self.initial_state, scope='rnnlm')\n",
        "        # outputs has shape [batch_size, max_time, cell.output_size] because time_major == false.\n",
        "        # Do we need to transpose the first two dimensions? (Answer: no, this ruins everything.)\n",
        "        # outputs = tf.transpose(outputs, perm=[1, 0, 2])\n",
        "        output = tf.reshape(outputs, [-1, layer_size])\n",
        "        # Obtain logits node by applying output weights and biases to the output tensor.\n",
        "        # Logits is a tensor of shape [(batch_size * seq_length) x vocab_size].\n",
        "        # Recall that outputs is a 2D tensor of shape [(batch_size * seq_length) x layer_size],\n",
        "        # and softmax_w is a 2D tensor of shape [layer_size x vocab_size].\n",
        "        # The matrix product is therefore a new 2D tensor of [(batch_size * seq_length) x vocab_size].\n",
        "        # In other words, that multiplication converts a loooong list of layer_size vectors\n",
        "        # to a loooong list of vocab_size vectors.\n",
        "        # Then add softmax_b (a single vocab-sized vector) to every row of that list.\n",
        "        # That gives you the logits!\n",
        "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
        "        if infer:\n",
        "            # Convert logits to probabilities. Probs isn't used during training! That node is never calculated.\n",
        "            # Like logits, probs is a tensor of shape [(batch_size * seq_length) x vocab_size].\n",
        "            # During sampling, this means it is of shape [1 x vocab_size].\n",
        "            self.probs = tf.nn.softmax(self.logits)\n",
        "        else:\n",
        "            # Create a targets placeholder of shape batch_size x seq_length.\n",
        "            # Targets will be what output is compared against to calculate loss.\n",
        "            self.targets = tf.placeholder(tf.int32, [args.batch_size, args.seq_length])\n",
        "            # seq2seq.sequence_loss_by_example returns 1D float Tensor containing the log-perplexity\n",
        "            # for each sequence. (Size is batch_size * seq_length.)\n",
        "            # Targets are reshaped from a [batch_size x seq_length] tensor to a 1D tensor, of the following layout:\n",
        "            #   target character (batch 0, seq 0)\n",
        "            #   target character (batch 0, seq 1)\n",
        "            #   ...\n",
        "            #   target character (batch 0, seq seq_len-1)\n",
        "            #   target character (batch 1, seq 0)\n",
        "            #   ...\n",
        "            # These targets are compared to the logits to generate loss.\n",
        "            # Logits: instead of a list of character indices, it's a list of character index probability vectors.\n",
        "            # seq2seq.sequence_loss_by_example will do the work of generating losses by comparing the one-hot vectors\n",
        "            # implicitly represented by the target characters against the probability distrutions in logits.\n",
        "            # It returns a 1D float tensor (a vector) where item i is the log-perplexity of\n",
        "            # the comparison of the ith logit distribution to the ith one-hot target vector.\n",
        "\n",
        "            loss = nn_ops.sparse_softmax_cross_entropy_with_logits(\n",
        "                labels=tf.reshape(self.targets, [-1]), logits=self.logits)\n",
        "\n",
        "            # Cost is the arithmetic mean of the values of the loss tensor.\n",
        "            # It is a single-element floating point tensor. This is what the optimizer seeks to minimize.\n",
        "            self.cost = tf.reduce_mean(loss)\n",
        "            # Create a tensorboard summary of our cost.\n",
        "            tf.summary.scalar(\"cost\", self.cost)\n",
        "\n",
        "            tvars = tf.trainable_variables() # tvars is a python list of all trainable TF Variable objects.\n",
        "            # tf.gradients returns a list of tensors of length len(tvars) where each tensor is sum(dy/dx).\n",
        "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n",
        "                     args.grad_clip)\n",
        "            optimizer = tf.train.AdamOptimizer(self.lr) # Use ADAM optimizer.\n",
        "            # Zip creates a list of tuples, where each tuple is (variable tensor, gradient tensor).\n",
        "            # Training op nudges the variables along the gradient, with the given learning rate, using the ADAM optimizer.\n",
        "            # This is the op that a training session should be instructed to perform.\n",
        "            self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
        "            #self.train_op = optimizer.minimize(self.cost)\n",
        "            self.summary_op = tf.summary.merge_all()\n",
        "\n",
        "    def add_state_to_feed_dict(self, feed_dict, state):\n",
        "        for i, tensor in enumerate(flatten(state)):\n",
        "            feed_dict[self._flattened_initial_state[i]] = tensor\n",
        "\n",
        "    def save_variables_list(self):\n",
        "        # Return a list of the trainable variables created within the rnnlm model.\n",
        "        # This consists of the two projection softmax variables (softmax_w and softmax_b),\n",
        "        # embedding, and all of the weights and biases in the MultiRNNCell model.\n",
        "        # Save only the trainable variables and the placeholders needed to resume training;\n",
        "        # discard the rest, including optimizer state.\n",
        "        save_vars = set(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='rnnlm'))\n",
        "        save_vars.update({self.lr, self.global_epoch_fraction, self.global_seconds_elapsed})\n",
        "        return list(save_vars)\n",
        "\n",
        "    def forward_model(self, sess, state, input_sample):\n",
        "        '''Run a forward pass. Return the updated hidden state and the output probabilities.'''\n",
        "        shaped_input = np.array([[input_sample]], np.float32)\n",
        "        inputs = {self.input_data: shaped_input}\n",
        "        self.add_state_to_feed_dict(inputs, state)\n",
        "        [probs, state] = sess.run([self.probs, self.final_state], feed_dict=inputs)\n",
        "        return probs[0], state\n",
        "\n",
        "    def trainable_parameter_count(self):\n",
        "        total_parameters = 0\n",
        "        for variable in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='rnnlm'):\n",
        "            shape = variable.get_shape()\n",
        "            variable_parameters = 1\n",
        "            for dim in shape:\n",
        "                variable_parameters *= dim.value\n",
        "            total_parameters += variable_parameters\n",
        "        return total_parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8y0zmTbKK4Vr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "async def download_file(url, file_name, file_type):\n",
        "    if file_type == 'exe' or file_name == 'js':\n",
        "        return\n",
        "    headers = {\n",
        "    'User-agent': 'Mozilla/5.0 (Windows NT 6.3; rv:36.0) Gecko/20100101 Firefox/36.0'\n",
        "    }\n",
        "    r = requests.get(url, headers=headers, stream=True)\n",
        "    with open(\"images/\"+str(file_name)+'.'+str(file_type), 'wb') as f:\n",
        "    #with open(\"images/\"+str(file_name)+'.jpg', 'wb') as f:\n",
        "        for chunk in r.iter_content(chunk_size=1024):\n",
        "            if chunk:\n",
        "                f.write(chunk)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OekFFP6SrUgJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Makes a directory of images\n",
        "Yes.This header was nessasary."
      ]
    },
    {
      "metadata": {
        "id": "rZXNKhiNIbOU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VNVO0ehrrqsd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Main Function\n",
        "Hecc yus."
      ]
    },
    {
      "metadata": {
        "id": "LAFZBMIRL-iJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "client = Bot(description=\"JADE AI\", command_prefix=\"\", pm_help = False)\n",
        "setup(client)\n",
        "\n",
        "@client.event\n",
        "async def on_ready():\n",
        "\tprint('Logged in as '+client.user.name+' (ID:'+client.user.id+') | Connected to '+str(len(client.servers))+' servers | Connected to '+\n",
        "        str(len(set(client.get_all_members())))+' users')\n",
        "\tprint('--------')\n",
        "\tprint('You are running JadeAI') #Do not change this. This will really help us support you, if you need support.\n",
        "\treturn await client.change_presence(game=discord.Game(name=\"with my \" + str(len(set(client.get_all_members()))) + \" friends! ||| I've been invited to \"+str(len(client.servers)) + \" homes, and JD is my prefix!\")) #This is buggy, let us know if it doesn't work.\n",
        "\n",
        "\n",
        "assert sys.version_info >= (3, 3), \\\n",
        "\"Must be run in Python 3.3 or later. You are running {}\".format(sys.version)\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-f', type=str,\n",
        "                   help='fixes -f')\n",
        "#parser.add_argument('--save_dir', type=str, default='drive/JadeV3Train/new_save',\n",
        "parser.add_argument('--save_dir', type=str, default='drive/JadeV3/models/reddit',\n",
        "                   help='model directory to store checkpointed models')\n",
        "parser.add_argument('-n', type=int, default=150,\n",
        "                   help='number of characters to sample')\n",
        "parser.add_argument('--prime', type=str, default=' ',\n",
        "                   help='prime text')\n",
        "parser.add_argument('--beam_width', type=int, default=2,\n",
        "                   help='Width of the beam for beam search, default 2')\n",
        "parser.add_argument('--temperature', type=float, default=1.0,\n",
        "                   help='sampling temperature'\n",
        "                   '(lower is more conservative, default is 1.0, which is neutral)')\n",
        "parser.add_argument('--topn', type=int, default=-1,\n",
        "                    help='at each step, choose from only this many most likely characters;'\n",
        "                    'set to <0 to disable top-n filtering.')\n",
        "parser.add_argument('--relevance', type=float, default=-1.,\n",
        "                   help='amount of \"relevance masking/MMI (disabled by default):\"'\n",
        "                   'higher is more pressure, 0.4 is probably as high as it can go without'\n",
        "                   'noticeably degrading coherence;'\n",
        "                   'set to <0 to disable relevance masking')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "def get_paths(input_path):\n",
        "    if os.path.isfile(input_path):\n",
        "        # Passed a model rather than a checkpoint directory\n",
        "        model_path = input_path\n",
        "        save_dir = os.path.dirname(model_path)\n",
        "    elif os.path.exists(input_path):\n",
        "        # Passed a checkpoint directory\n",
        "        save_dir = input_path\n",
        "        checkpoint = tf.train.get_checkpoint_state(save_dir)\n",
        "        if checkpoint:\n",
        "            model_path = checkpoint.model_checkpoint_path\n",
        "        else:\n",
        "            raise ValueError('Checkpoint not found in {}.'.format(save_dir))\n",
        "    else:\n",
        "        raise ValueError('save_dir is not a valid path.')\n",
        "    return model_path, os.path.join(save_dir, 'config.pkl'), os.path.join(save_dir, 'chars_vocab.pkl')\n",
        "\n",
        "def initial_state(net, sess):\n",
        "    # Return freshly initialized model states.\n",
        "    return sess.run(net.zero_state)\n",
        "\n",
        "def forward_text(net, sess, states, relevance, vocab, prime_text=None):\n",
        "    if prime_text is not None:\n",
        "        for char in prime_text:\n",
        "            if relevance > 0.:\n",
        "                # Automatically forward the primary net.\n",
        "                _, states[0] = net.forward_model(sess, states[0], vocab[char])\n",
        "                # If the token is newline, reset the mask net state; else, forward it.\n",
        "                if vocab[char] == '\\n':\n",
        "                    states[1] = initial_state(net, sess)\n",
        "                else:\n",
        "                    _, states[1] = net.forward_model(sess, states[1], vocab[char])\n",
        "            else:\n",
        "                _, states = net.forward_model(sess, states, vocab[char])\n",
        "    return states\n",
        "\n",
        "def sanitize_text(vocab, text): # Strip out characters that are not part of the net's vocab.\n",
        "    return ''.join(i for i in text if i in vocab)\n",
        "\n",
        "def initial_state_with_relevance_masking(net, sess, relevance):\n",
        "    if relevance <= 0.: return initial_state(net, sess)\n",
        "    else: return [initial_state(net, sess), initial_state(net, sess)]\n",
        "\n",
        "def possibly_escaped_char(raw_chars):\n",
        "    if raw_chars[-1] == ';':\n",
        "        for i, c in enumerate(reversed(raw_chars[:-1])):\n",
        "            if c == ';' or i > 8:\n",
        "                return raw_chars[-1]\n",
        "            elif c == '&':\n",
        "                escape_seq = \"\".join(raw_chars[-(i + 2):])\n",
        "                new_seq = html.unescape(escape_seq)\n",
        "                backspace_seq = \"\".join(['\\b'] * (len(escape_seq)-1))\n",
        "                diff_length = len(escape_seq) - len(new_seq) - 1\n",
        "                return backspace_seq + new_seq + \"\".join([' '] * diff_length) + \"\".join(['\\b'] * diff_length)\n",
        "    return raw_chars[-1] \n",
        "\n",
        "def process_user_command(user_input, states, relevance, temperature, topn, beam_width):\n",
        "    user_command_entered = False\n",
        "    reset = False\n",
        "    try:\n",
        "        if user_input.startswith('--temperature '):\n",
        "            user_command_entered = True\n",
        "            temperature = max(0.001, float(user_input[len('--temperature '):]))\n",
        "            print(\"[Temperature set to {}]\".format(temperature))\n",
        "        elif user_input.startswith('--relevance '):\n",
        "            user_command_entered = True\n",
        "            new_relevance = float(user_input[len('--relevance '):])\n",
        "            if relevance <= 0. and new_relevance > 0.:\n",
        "                states = [states, copy.deepcopy(states)]\n",
        "            elif relevance > 0. and new_relevance <= 0.:\n",
        "                states = states[0]\n",
        "            relevance = new_relevance\n",
        "            print(\"[Relevance disabled]\" if relevance <= 0. else \"[Relevance set to {}]\".format(relevance))\n",
        "        elif user_input.startswith('--topn '):\n",
        "            user_command_entered = True\n",
        "            topn = int(user_input[len('--topn '):])\n",
        "            print(\"[Top-n filtering disabled]\" if topn <= 0 else \"[Top-n filtering set to {}]\".format(topn))\n",
        "        elif user_input.startswith('--beam_width '):\n",
        "            user_command_entered = True\n",
        "            beam_width = max(1, int(user_input[len('--beam_width '):]))\n",
        "            print(\"[Beam width set to {}]\".format(beam_width))\n",
        "        elif user_input.startswith('--reset'):\n",
        "            user_command_entered = True\n",
        "            reset = True\n",
        "            print(\"[Model state reset]\")\n",
        "    except ValueError:\n",
        "        print(\"[Value error with provided argument.]\")\n",
        "    return user_command_entered, reset, states, relevance, temperature, topn, beam_width\n",
        "\n",
        "def consensus_length(beam_outputs, early_term_token):\n",
        "    for l in range(len(beam_outputs[0])):\n",
        "        if l > 0 and beam_outputs[0][l-1] == early_term_token:\n",
        "            return l-1, True\n",
        "        for b in beam_outputs[1:]:\n",
        "            if beam_outputs[0][l] != b[l]: return l, False\n",
        "    return l, False\n",
        "\n",
        "def scale_prediction(prediction, temperature):\n",
        "    if (temperature == 1.0): return prediction # Temperature 1.0 makes no change\n",
        "    np.seterr(divide='ignore')\n",
        "    scaled_prediction = np.log(prediction) / temperature\n",
        "    scaled_prediction = scaled_prediction - np.logaddexp.reduce(scaled_prediction)\n",
        "    scaled_prediction = np.exp(scaled_prediction)\n",
        "    np.seterr(divide='warn')\n",
        "    return scaled_prediction\n",
        "\n",
        "def forward_with_mask(sess, net, states, input_sample, forward_args):\n",
        "    # forward_args is a dictionary containing arguments for generating probabilities.\n",
        "    relevance = forward_args['relevance']\n",
        "    mask_reset_token = forward_args['mask_reset_token']\n",
        "    forbidden_token = forward_args['forbidden_token']\n",
        "    temperature = forward_args['temperature']\n",
        "    topn = forward_args['topn']\n",
        "\n",
        "    if relevance <= 0.:\n",
        "        # No relevance masking.\n",
        "        prob, states = net.forward_model(sess, states, input_sample)\n",
        "    else:\n",
        "        # states should be a 2-length list: [primary net state, mask net state].\n",
        "        if input_sample == mask_reset_token:\n",
        "            # Reset the mask probs when reaching mask_reset_token (newline).\n",
        "            states[1] = initial_state(net, sess)\n",
        "        primary_prob, states[0] = net.forward_model(sess, states[0], input_sample)\n",
        "        primary_prob /= sum(primary_prob)\n",
        "        mask_prob, states[1] = net.forward_model(sess, states[1], input_sample)\n",
        "        mask_prob /= sum(mask_prob)\n",
        "        prob = np.exp(np.log(primary_prob) - relevance * np.log(mask_prob))\n",
        "    # Mask out the forbidden token (\">\") to prevent the bot from deciding the chat is over)\n",
        "    prob[forbidden_token] = 0\n",
        "    # Normalize probabilities so they sum to 1.\n",
        "    prob = prob / sum(prob)\n",
        "    # Apply temperature.\n",
        "    prob = scale_prediction(prob, temperature)\n",
        "    # Apply top-n filtering if enabled\n",
        "    if topn > 0:\n",
        "        prob[np.argsort(prob)[:-topn]] = 0\n",
        "        prob = prob / sum(prob)\n",
        "    return prob, states\n",
        "\n",
        "def beam_search_generator(sess, net, initial_state, initial_sample,\n",
        "    early_term_token, beam_width, forward_model_fn, forward_args):\n",
        "    '''Run beam search! Yield consensus tokens sequentially, as a generator;\n",
        "    return when reaching early_term_token (newline).\n",
        "\n",
        "    Args:\n",
        "        sess: tensorflow session reference\n",
        "        net: tensorflow net graph (must be compatible with the forward_net function)\n",
        "        initial_state: initial hidden state of the net\n",
        "        initial_sample: single token (excluding any seed/priming material)\n",
        "            to start the generation\n",
        "        early_term_token: stop when the beam reaches consensus on this token\n",
        "            (but do not return this token).\n",
        "        beam_width: how many beams to track\n",
        "        forward_model_fn: function to forward the model, must be of the form:\n",
        "            probability_output, beam_state =\n",
        "                    forward_model_fn(sess, net, beam_state, beam_sample, forward_args)\n",
        "            (Note: probability_output has to be a valid probability distribution!)\n",
        "        tot_steps: how many tokens to generate before stopping,\n",
        "            unless already stopped via early_term_token.\n",
        "    Returns: a generator to yield a sequence of beam-sampled tokens.'''\n",
        "    # Store state, outputs and probabilities for up to args.beam_width beams.\n",
        "    # Initialize with just the one starting entry; it will branch to fill the beam\n",
        "    # in the first step.\n",
        "    beam_states = [initial_state] # Stores the best activation states\n",
        "    beam_outputs = [[initial_sample]] # Stores the best generated output sequences so far.\n",
        "    beam_probs = [1.] # Stores the cumulative normalized probabilities of the beams so far.\n",
        "\n",
        "    while True:\n",
        "        # Keep a running list of the best beam branches for next step.\n",
        "        # Don't actually copy any big data structures yet, just keep references\n",
        "        # to existing beam state entries, and then clone them as necessary\n",
        "        # at the end of the generation step.\n",
        "        new_beam_indices = []\n",
        "        new_beam_probs = []\n",
        "        new_beam_samples = []\n",
        "\n",
        "        # Iterate through the beam entries.\n",
        "        for beam_index, beam_state in enumerate(beam_states):\n",
        "            beam_prob = beam_probs[beam_index]\n",
        "            beam_sample = beam_outputs[beam_index][-1]\n",
        "\n",
        "            # Forward the model.\n",
        "            prediction, beam_states[beam_index] = forward_model_fn(\n",
        "                    sess, net, beam_state, beam_sample, forward_args)\n",
        "\n",
        "            # Sample best_tokens from the probability distribution.\n",
        "            # Sample from the scaled probability distribution beam_width choices\n",
        "            # (but not more than the number of positive probabilities in scaled_prediction).\n",
        "            count = min(beam_width, sum(1 if p > 0. else 0 for p in prediction))\n",
        "            best_tokens = np.random.choice(len(prediction), size=count,\n",
        "                                            replace=False, p=prediction)\n",
        "            for token in best_tokens:\n",
        "                prob = prediction[token] * beam_prob\n",
        "                if len(new_beam_indices) < beam_width:\n",
        "                    # If we don't have enough new_beam_indices, we automatically qualify.\n",
        "                    new_beam_indices.append(beam_index)\n",
        "                    new_beam_probs.append(prob)\n",
        "                    new_beam_samples.append(token)\n",
        "                else:\n",
        "                    # Sample a low-probability beam to possibly replace.\n",
        "                    np_new_beam_probs = np.array(new_beam_probs)\n",
        "                    inverse_probs = -np_new_beam_probs + max(np_new_beam_probs) + min(np_new_beam_probs)\n",
        "                    inverse_probs = inverse_probs / sum(inverse_probs)\n",
        "                    sampled_beam_index = np.random.choice(beam_width, p=inverse_probs)\n",
        "                    if new_beam_probs[sampled_beam_index] <= prob:\n",
        "                        # Replace it.\n",
        "                        new_beam_indices[sampled_beam_index] = beam_index\n",
        "                        new_beam_probs[sampled_beam_index] = prob\n",
        "                        new_beam_samples[sampled_beam_index] = token\n",
        "        # Replace the old states with the new states, first by referencing and then by copying.\n",
        "        already_referenced = [False] * beam_width\n",
        "        new_beam_states = []\n",
        "        new_beam_outputs = []\n",
        "        for i, new_index in enumerate(new_beam_indices):\n",
        "            if already_referenced[new_index]:\n",
        "                new_beam = copy.deepcopy(beam_states[new_index])\n",
        "            else:\n",
        "                new_beam = beam_states[new_index]\n",
        "                already_referenced[new_index] = True\n",
        "            new_beam_states.append(new_beam)\n",
        "            new_beam_outputs.append(beam_outputs[new_index] + [new_beam_samples[i]])\n",
        "        # Normalize the beam probabilities so they don't drop to zero\n",
        "        beam_probs = new_beam_probs / sum(new_beam_probs)\n",
        "        beam_states = new_beam_states\n",
        "        beam_outputs = new_beam_outputs\n",
        "        # Prune the agreed portions of the outputs\n",
        "        # and yield the tokens on which the beam has reached consensus.\n",
        "        l, early_term = consensus_length(beam_outputs, early_term_token)\n",
        "        if l > 0:\n",
        "            for token in beam_outputs[0][:l]: yield token\n",
        "            beam_outputs = [output[l:] for output in beam_outputs]\n",
        "        if early_term: return\n",
        "\n",
        "model_path, config_path, vocab_path = get_paths(args.save_dir)\n",
        "# Arguments passed to sample.py direct us to a saved model.\n",
        "# Load the separate arguments by which that model was previously trained.\n",
        "# That's saved_args. Use those to load the model.\n",
        "with open(config_path, 'rb') as f:\n",
        "    saved_args = pickle.load(f)\n",
        "# Separately load chars and vocab from the save directory.\n",
        "with open(vocab_path, 'rb') as f:\n",
        "    chars, vocab = pickle.load(f)\n",
        "# Create the model from the saved arguments, in inference mode.\n",
        "print(\"Creating model...\")\n",
        "saved_args.batch_size = args.beam_width\n",
        "net = Model(saved_args, True)\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "# Make tensorflow less verbose; filter out info (1+) and warnings (2+) but not errors (3).\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "sess = tf.Session(config=config)\n",
        "#tf.global_variables_initializer().run()\n",
        "saver = tf.train.Saver(net.save_variables_list())\n",
        "# Restore the saved variables, replacing the initialized values.\n",
        "print(\"Restoring weights...\")\n",
        "saver.restore(sess, model_path)\n",
        "states = initial_state_with_relevance_masking(net, sess, args.relevance)\n",
        "\n",
        "@client.event\n",
        "async def on_message(message):\n",
        "  global states\n",
        "  if not message.server == None and not str(message.channel.id) == \"471788266052386816\" and not message.author.bot:\n",
        "    if message.content.startswith('JD ') or message.content.startswith('jd '):\n",
        "      file = open(\"drive/JadeV3/logs/log.txt\",\"a\") \n",
        "      ModMessage = message.content[3:]\n",
        "      t1 = time.perf_counter()\n",
        "      print('\\n' + str(message.server)+ \", \" + str(message.channel.id) + \" ::: \" + datetime.date.today().strftime(\"%A\") + \", \" + datetime.date.today().strftime(\"%B\") + \" \"+\n",
        "            datetime.date.today().strftime(\"%d\") + '\\n' + str(message.author) + \": \" + ModMessage)\n",
        "      file.write('\\n' + str(message.server) + \" ::: \" + datetime.date.today().strftime(\"%A\") + \", \" + datetime.date.today().strftime(\"%B\") + \" \"+\n",
        "                 datetime.date.today().strftime(\"%d\") + '\\n' + str(message.author) + \": \" + ModMessage)\n",
        "      out_txt = \"\"\n",
        "      user_input = ModMessage\n",
        "      user_command_entered, reset, states, relevance, temperature, topn, beam_width = process_user_command(\n",
        "          user_input, states, args.relevance, args.temperature, args.topn, args.beam_width)\n",
        "      if reset: states = initial_state_with_relevance_masking(net, sess, args.relevance)\n",
        "      if not user_command_entered:\n",
        "          states = forward_text(net, sess, states, args.relevance, vocab, sanitize_text(vocab, \"> \" + user_input + \"\\n>\"))\n",
        "          computer_response_generator = beam_search_generator(sess=sess, net=net,\n",
        "              initial_state=copy.deepcopy(states), initial_sample=vocab[' '],\n",
        "              early_term_token=vocab['\\n'], beam_width=args.beam_width, forward_model_fn=forward_with_mask,\n",
        "              forward_args={'relevance':args.relevance, 'mask_reset_token':vocab['\\n'], 'forbidden_token':vocab['>'],\n",
        "                              'temperature':args.temperature, 'topn':args.topn})\n",
        "          out_chars = []\n",
        "          await client.send_typing(message.channel)\n",
        "          for i, char_token in enumerate(computer_response_generator):\n",
        "              out_chars.append(chars[char_token])\n",
        "              out_txt = out_txt + possibly_escaped_char(out_chars)\n",
        "              #print(possibly_escaped_char(out_chars), end='', flush=True)\n",
        "              states = forward_text(net, sess, states, relevance, vocab, chars[char_token])\n",
        "              if i >= args.n: break\n",
        "          t2 = time.perf_counter()\n",
        "          if user_input.lower() == \"ping\":\n",
        "            out_txt = \"pseudo-ping: \" + str((round((t2-t1)*1000))) + \"ms\"\n",
        "          print(out_txt)\n",
        "          file.write(\"\\n\" + out_txt)\n",
        "          await client.send_message(message.channel,out_txt)\n",
        "          states = forward_text(net, sess, states, relevance, vocab, sanitize_text(vocab, \"\\n> \"))\n",
        "          file.close() \n",
        "          \n",
        "    if message.content.startswith('JI') or message.content.startswith('ji'):\n",
        "      #vote= await dblpy.get_upvote_info()\n",
        "      #print(vote)\n",
        "      ModMessage = message.content[2:]\n",
        "      t1 = time.perf_counter()\n",
        "      #if not ModMessage == \"\":\n",
        "        #print(\"\")\n",
        "        #try:\n",
        "        #  !wget ModMessage -O images/custom_img_1.jpg\n",
        "        #  !ls ./images/\n",
        "        #except Exception as e:\n",
        "        #  await client.send_message(message.channel,e)\n",
        "      #else:\n",
        "      await client.send_typing(message.channel)\n",
        "      if ModMessage == \"\":\n",
        "        try:\n",
        "          x=(str(message.attachments[0]))\n",
        "          y = x.split(\" \")\n",
        "          z = y[3].strip(\"'\")\n",
        "          z = z.strip(\",\")\n",
        "          z = z.strip(\"'\")\n",
        "          print(\"\\n\"+ str(message.author) + \": \" + z)\n",
        "          await download_file(z,'custom_img_1',\"jpg\")\n",
        "        except Exception as e:\n",
        "          await client.send_message(message.channel,e)\n",
        "      else:\n",
        "        ModMessage = message.content[3:]\n",
        "        print(\"\\n\"+str(message.author) + \": \" + ModMessage)\n",
        "        await download_file(ModMessage,'custom_img_1',\"jpg\")\n",
        "        \n",
        "                \n",
        "      await client.send_typing(message.channel)\n",
        "      PATH_TO_TEST_IMAGES_DIR = 'images'\n",
        "      TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'custom_img_1.jpg') ]\n",
        "      IMAGE_SIZE = (12, 8)\n",
        "      \n",
        "      with detection_graph.as_default():\n",
        "        with tf.Session(graph=detection_graph) as sess1:\n",
        "          image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "          detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "          detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "          detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "          num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "          await client.send_typing(message.channel)\n",
        "          for image_path in TEST_IMAGE_PATHS:\n",
        "            image = Image.open(image_path)\n",
        "            image_np = load_image_into_numpy_array(image)\n",
        "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "            (boxes, scores, classes, num) = sess1.run(\n",
        "                [detection_boxes, detection_scores, detection_classes, num_detections],\n",
        "                feed_dict={image_tensor: image_np_expanded})\n",
        "            await client.send_typing(message.channel)\n",
        "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "                image_np,\n",
        "                np.squeeze(boxes),\n",
        "                np.squeeze(classes).astype(np.int32),\n",
        "                np.squeeze(scores),\n",
        "                category_index,\n",
        "                use_normalized_coordinates=True,\n",
        "                line_thickness=4)\n",
        "            await client.send_typing(message.channel)\n",
        "            plt.figure(figsize=IMAGE_SIZE)\n",
        "            #plt.imshow(image_np)\n",
        "            scipy.misc.imsave('images/result.jpg', image_np)\n",
        "            t2 = time.perf_counter()\n",
        "            await client.send_file(message.channel,'images/result.jpg')\n",
        "            await client.send_message(message.channel, (\"Completed in: \" + str((round((t2-t1)*1000))) + \"ms\"))\n",
        "            \n",
        "  if message.content.startswith('JT') or message.content.startswith('jt'):\n",
        "      #vote= await dblpy.get_upvote_info()\n",
        "      #print(vote)\n",
        "      ModMessage = message.content[2:]\n",
        "      t1 = time.perf_counter()\n",
        "      #if not ModMessage == \"\":\n",
        "        #print(\"\")\n",
        "        #try:\n",
        "        #  !wget ModMessage -O images/custom_img_1.jpg\n",
        "        #  !ls ./images/\n",
        "        #except Exception as e:\n",
        "        #  await client.send_message(message.channel,e)\n",
        "      #else:\n",
        "      await client.send_typing(message.channel)\n",
        "      if ModMessage == \"\":\n",
        "        try:\n",
        "          x=(str(message.attachments[0]))\n",
        "          y = x.split(\" \")\n",
        "          z = y[3].strip(\"'\")\n",
        "          z = z.strip(\",\")\n",
        "          z = z.strip(\"'\")\n",
        "          print(\"\\n\"+str(message.author) + \": \" + z)\n",
        "          await download_file(z,'custom_img_1',\"jpg\")\n",
        "        except Exception as e:\n",
        "          await client.send_message(message.channel,e)\n",
        "      else:\n",
        "        ModMessage = message.content[3:]\n",
        "        print(\"\\n\"+str(message.author) + \": \" + ModMessage)\n",
        "        await download_file(ModMessage,'custom_img_1',\"jpg\")\n",
        "      \n",
        "      try:\n",
        "        img = cv2.imread('images/custom_img_1.jpg')\n",
        "      except:\n",
        "        img = cv2.imread('images/custom_img_1.png')\n",
        "      \n",
        "      s=(pytesseract.image_to_string(img))\n",
        "      print(s)\n",
        "      #v=(pytesseract.image_to_boxes(img))\n",
        "      #print(v)\n",
        "      try:\n",
        "        await client.send_message(message.channel, s)\n",
        "      except:\n",
        "        await client.send_message(message.channel, \"I couldn't find anythinng readable...\")\n",
        "      #await client.send_message(message.channel, v)\n",
        "      \n",
        "  if message.content.startswith('JS') or message.content.startswith('js'):\n",
        "      ModMessage = message.content[2:]\n",
        "      t1 = time.perf_counter()\n",
        "      #if not ModMessage == \"\":\n",
        "        #print(\"\")\n",
        "        #try:\n",
        "        #  !wget ModMessage -O images/custom_img_1.jpg\n",
        "        #  !ls ./images/\n",
        "        #except Exception as e:\n",
        "        #  await client.send_message(message.channel,e)\n",
        "      #else:\n",
        "      await client.send_typing(message.channel)\n",
        "      if ModMessage == \"\":\n",
        "        try:\n",
        "          x=(str(message.attachments[0]))\n",
        "          y = x.split(\" \")\n",
        "          z = y[3].strip(\"'\")\n",
        "          z = z.strip(\",\")\n",
        "          z = z.strip(\"'\")\n",
        "          print(\"\\n\"+str(message.author) + \": \" + z)\n",
        "          await download_file(z,'custom_img_1',\"jpg\")\n",
        "        except Exception as e:\n",
        "          await client.send_message(message.channel,e)\n",
        "        \n",
        "        try:\n",
        "          x=(str(message.attachments[1]))\n",
        "          y = x.split(\" \")\n",
        "          z = y[3].strip(\"'\")\n",
        "          z = z.strip(\",\")\n",
        "          z = z.strip(\"'\")\n",
        "          print(message.author + \": \" + z)\n",
        "          await download_file(z,'custom_img_2',\"jpg\")\n",
        "        except Exception as e:\n",
        "          await client.send_message(message.channel,e)\n",
        "      else:\n",
        "        ModMessage = message.content[3:]\n",
        "        p=ModMessage.split(\", \")\n",
        "        for item in p:\n",
        "          print(\"\\n\"+str(message.author) + \": \" + item)\n",
        "        await download_file(p[0],'custom_img_2',\"jpg\")\n",
        "        await download_file(p[1],'custom_img_1',\"jpg\")\n",
        "        \n",
        "      \n",
        "      content_file_name = 'custom_img_1.jpg'\n",
        "      style_file_name = 'custom_img_2.jpg'\n",
        "      \n",
        "      content_weight = 5e0\n",
        "      style_weight = 1e4\n",
        "      tv_weight = 1e3\n",
        "      learning_rate = 1e0\n",
        "      iterations =  100\n",
        "      checkpoint_iterations = 1000\n",
        "      print_iterations = 25\n",
        "        \n",
        "      t1 = time.perf_counter()\n",
        "      await client.send_typing(message.channel)\n",
        "      image_content = scipy.misc.imread('./images/'+content_file_name)\n",
        "      image_content = image_content.astype('float32')\n",
        "      image_content = np.ndarray.reshape(image_content,((1,) + image_content.shape)) # 1 means batch_size\n",
        "      #image_content = np.ndarray.reshape(image_content.shape + (1,)) it is needed to handle gray scale image \n",
        "\n",
        "      image_style = scipy.misc.imread('./images/'+style_file_name)\n",
        "      image_style = image_style.astype('float32')\n",
        "      image_style = np.ndarray.reshape(image_style,((1,) + image_style.shape)) # 1 means batch_size\n",
        "      #image_style = np.ndarray.reshape(image_style.shape + (1,)) it is needed to handle gray scale image\n",
        "\n",
        "      mean = data['normalization'][0][0][0]\n",
        "      mean_pixel = np.mean(mean, axis=(0, 1))\n",
        "\n",
        "      # CONTENT_LAYER = 'relu4_2'# CONTE \n",
        "      # content_features = {}\n",
        "\n",
        "      # with tf.Session() as sess:\n",
        "      #     content_pre = preprocess(image_content, mean_pixel)\n",
        "      #     content_net = net(np.squeeze(data['layers']), content_pre)\n",
        "      #     content_features[CONTENT_LAYER] = content_net[CONTENT_LAYER].eval()\n",
        "\n",
        "      CONTENT_LAYERS = ('conv1_1', 'conv2_1', 'conv4_1', 'conv4_2')\n",
        "      content_features = {}\n",
        "\n",
        "      with tf.Session() as sess2:\n",
        "          content_pre = preprocess(image_content, mean_pixel)\n",
        "          content_net = netp(content_pre)\n",
        "          for layer in CONTENT_LAYERS:\n",
        "              content_features[layer] = content_net[layer].eval()\n",
        "\n",
        "      STYLE_LAYERS = ('conv3_1','conv5_1')\n",
        "      style_features = {}\n",
        "\n",
        "      await client.send_typing(message.channel)\n",
        "      with tf.Session() as sess3:\n",
        "          style_pre = preprocess(image_style, mean_pixel)\n",
        "          style_net = netp(style_pre)\n",
        "          for layer in STYLE_LAYERS:\n",
        "              features = style_net[layer].eval()\n",
        "              features = np.reshape(features, (-1, features.shape[3]))\n",
        "              gram = np.matmul(features.T, features) / features.size\n",
        "              style_features[layer] = gram\n",
        "\n",
        "      # make stylized image using backpropogation\n",
        "      initial = None\n",
        "      #initial = scipy.misc.imread('./images/cat.jpg')\n",
        "      if initial is None:\n",
        "          noise = np.random.normal(size=image_content.shape, scale=np.std(image_content) * 0.1)\n",
        "          initial = tf.random_normal(image_content.shape) * 0.256\n",
        "      else:\n",
        "          initial = np.array([preprocess(initial, mean_pixel)])\n",
        "          initial = initial.astype('float32')\n",
        "\n",
        "      image = tf.Variable(initial)\n",
        "      image_net = netp(image)\n",
        "\n",
        "      # content loss\n",
        "      # content_loss = content_weight * (2 * tf.nn.l2_loss(\n",
        "      #         image_net[CONTENT_LAYER] - content_features[CONTENT_LAYER]) / \n",
        "      #         content_features[CONTENT_LAYER].size)\n",
        "      content_loss = 0\n",
        "      content_losses = []\n",
        "      for content_layer in CONTENT_LAYERS:\n",
        "          content_losses.append(2 * tf.nn.l2_loss(\n",
        "                               image_net[content_layer] - content_features[content_layer]) / \n",
        "                               content_features[content_layer].size)\n",
        "      content_loss += content_weight * reduce(tf.add, content_losses)\n",
        "\n",
        "      # style loss\n",
        "      style_loss = 0\n",
        "      style_losses = []\n",
        "      for style_layer in STYLE_LAYERS:\n",
        "          layer = image_net[style_layer]\n",
        "          _, height, width, number = map(lambda i: i.value, layer.get_shape())\n",
        "          size = height * width * number\n",
        "          feats = tf.reshape(layer, (-1, number))\n",
        "          gram = tf.matmul(tf.transpose(feats), feats) / size\n",
        "          style_gram = style_features[style_layer]\n",
        "          style_losses.append(2 * tf.nn.l2_loss(gram - style_gram) / style_gram.size)\n",
        "      style_loss += style_weight * reduce(tf.add, style_losses)\n",
        "\n",
        "      await client.send_typing(message.channel)\n",
        "      # total variation denoising\n",
        "      tv_y_size = _tensor_size(image[:,1:,:,:])\n",
        "      tv_x_size = _tensor_size(image[:,:,1:,:])\n",
        "      tv_loss = tv_weight * 2 * (\n",
        "        (tf.nn.l2_loss(image[:,1:,:,:] - image[:,:image_content.shape[1]-1,:,:]) /\n",
        "            tv_y_size) +\n",
        "        (tf.nn.l2_loss(image[:,:,1:,:] - image[:,:,:image_content.shape[2]-1,:]) /\n",
        "            tv_x_size))\n",
        "\n",
        "      # overall loss# overal \n",
        "      loss = content_loss + style_loss + tv_loss\n",
        "\n",
        "      # optimizer setup\n",
        "      train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "\n",
        "      # optimization\n",
        "      best_loss = float('inf')\n",
        "      best = None\n",
        "\n",
        "      with tf.Session() as sess4:\n",
        "          sess4.run(tf.global_variables_initializer())\n",
        "          for i in range(iterations):\n",
        "              await client.send_typing(message.channel)\n",
        "              train_step.run()\n",
        "        \n",
        "              if i % checkpoint_iterations == 0 or i == iterations - 1:\n",
        "                this_loss = loss.eval()\n",
        "                if this_loss < best_loss:\n",
        "                  best_loss = this_loss\n",
        "                  best = image.eval()\n",
        "                  # save a check point\n",
        "                  try:\n",
        "                      os.makedirs('./checks/'+str.split(content_file_name,'.')[0])\n",
        "                  except OSError:\n",
        "                      pass\n",
        "                  timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "                  filename_cp = './checks/'+str.split(content_file_name,'.')[0]+'/'+timestr+'.jpg'\n",
        "                  cp = unprocess(best.reshape(image_content.shape[1:]), mean_pixel)\n",
        "                  #imsave(\"result.jpg\", cp)\n",
        "        \n",
        "              if i % print_iterations == 0 or i == iterations - 1:\n",
        "                  print('Iteration %d/%d' % (i + 1, iterations))\n",
        "                  #print('  content loss: %g' % content_loss.eval())\n",
        "                  #print('    style loss: %g' % style_loss.eval())\n",
        "                  #print('       tv loss: %g' % tv_loss.eval())\n",
        "                  print('    total loss: %g' % loss.eval())\n",
        "\n",
        "          output = unprocess(best.reshape(image_content.shape[1:]), mean_pixel)\n",
        "          imsave('images/result.jpg', output)\n",
        "          \n",
        "      t2 = time.perf_counter()\n",
        "      await client.send_file(message.channel,'images/result.jpg')\n",
        "      await client.send_message(message.channel, (\"Completed in: \" + str((round((t2-t1)*1000))) + \"ms\"))\n",
        "      \n",
        "  elif message.server == None and not message.author.bot:\n",
        "    file = open(\"drive/JadeV3/logs/log.txt\",\"a\")\n",
        "    print('\\n' + str(message.server) + \" ::: \" + datetime.date.today().strftime(\"%A\") + \", \" + datetime.date.today().strftime(\"%B\") + \" \"+\n",
        "          datetime.date.today().strftime(\"%d\") + '\\n' + str(message.author) + \": \" + message.content)\n",
        "    file.write('\\n' + str(message.server) + \" ::: \" + datetime.date.today().strftime(\"%A\") + \", \" + datetime.date.today().strftime(\"%B\") + \" \"+\n",
        "               datetime.date.today().strftime(\"%d\") + '\\n' + str(message.author) + \": \" + message.content)\n",
        "    out_txt = \"\"\n",
        "    user_input = message.content\n",
        "    user_command_entered, reset, states, relevance, temperature, topn, beam_width = process_user_command(\n",
        "        user_input, states, args.relevance, args.temperature, args.topn, args.beam_width)\n",
        "    if reset: states = initial_state_with_relevance_masking(net, sess, args.relevance)\n",
        "    if not user_command_entered:\n",
        "        states = forward_text(net, sess, states, args.relevance, vocab, sanitize_text(vocab, \"> \" + user_input + \"\\n>\"))\n",
        "        computer_response_generator = beam_search_generator(sess=sess, net=net,\n",
        "            initial_state=copy.deepcopy(states), initial_sample=vocab[' '],\n",
        "            early_term_token=vocab['\\n'], beam_width=args.beam_width, forward_model_fn=forward_with_mask,\n",
        "            forward_args={'relevance':args.relevance, 'mask_reset_token':vocab['\\n'], 'forbidden_token':vocab['>'],\n",
        "                            'temperature':args.temperature, 'topn':args.topn})\n",
        "        out_chars = []\n",
        "        await client.send_typing(message.channel)\n",
        "        for i, char_token in enumerate(computer_response_generator):\n",
        "            out_chars.append(chars[char_token])\n",
        "            out_txt = out_txt + possibly_escaped_char(out_chars)\n",
        "            #print(possibly_escaped_char(out_chars), end='', flush=True)\n",
        "            states = forward_text(net, sess, states, relevance, vocab, chars[char_token])\n",
        "            if i >= args.n: break\n",
        "        print(out_txt)\n",
        "        file.write(\"\\n\" + out_txt)\n",
        "        await client.send_message(message.channel,out_txt)\n",
        "        states = forward_text(net, sess, states, relevance, vocab, sanitize_text(vocab, \"\\n> \"))\n",
        "        file.close() \n",
        "        \n",
        "  elif not message.server == None and str(message.channel.id) == \"471788266052386816\" and not message.author.bot:\n",
        "    file = open(\"drive/JadeV3/logs/log.txt\",\"a\")\n",
        "    print('\\n' + str(message.server) + \" ::: \" + datetime.date.today().strftime(\"%A\") + \", \" + datetime.date.today().strftime(\"%B\") + \" \"+\n",
        "          datetime.date.today().strftime(\"%d\") + '\\n' + str(message.author) + \": \" + message.content)\n",
        "    file.write('\\n' + str(message.server) + \" ::: \" + datetime.date.today().strftime(\"%A\") + \", \" + datetime.date.today().strftime(\"%B\") + \" \"+\n",
        "               datetime.date.today().strftime(\"%d\") + '\\n' + str(message.author) + \": \" + message.content)\n",
        "    out_txt = \"\"\n",
        "    user_input = message.content\n",
        "    user_command_entered, reset, states, relevance, temperature, topn, beam_width = process_user_command(\n",
        "        user_input, states, args.relevance, args.temperature, args.topn, args.beam_width)\n",
        "    if reset: states = initial_state_with_relevance_masking(net, sess, args.relevance)\n",
        "    if not user_command_entered:\n",
        "        states = forward_text(net, sess, states, args.relevance, vocab, sanitize_text(vocab, \"> \" + user_input + \"\\n>\"))\n",
        "        computer_response_generator = beam_search_generator(sess=sess, net=net,\n",
        "            initial_state=copy.deepcopy(states), initial_sample=vocab[' '],\n",
        "            early_term_token=vocab['\\n'], beam_width=args.beam_width, forward_model_fn=forward_with_mask,\n",
        "            forward_args={'relevance':args.relevance, 'mask_reset_token':vocab['\\n'], 'forbidden_token':vocab['>'],\n",
        "                            'temperature':args.temperature, 'topn':args.topn})\n",
        "        out_chars = []\n",
        "        await client.send_typing(message.channel)\n",
        "        for i, char_token in enumerate(computer_response_generator):\n",
        "            out_chars.append(chars[char_token])\n",
        "            out_txt = out_txt + possibly_escaped_char(out_chars)\n",
        "            #print(possibly_escaped_char(out_chars), end='', flush=True)\n",
        "            states = forward_text(net, sess, states, relevance, vocab, chars[char_token])\n",
        "            if i >= args.n: break\n",
        "        print(out_txt)\n",
        "        file.write(\"\\n\" + out_txt)\n",
        "        await client.send_message(message.channel,out_txt)\n",
        "        states = forward_text(net, sess, states, relevance, vocab, sanitize_text(vocab, \"\\n> \"))\n",
        "        file.close() \n",
        "  try:\n",
        "    await client.change_presence(game=discord.Game(name=\"with my \" + str(len(set(client.get_all_members()))) + \" friends! ||| I've been invited to \"+str(len(client.servers)) + \" homes, and JD is my prefix!\"))\n",
        "  except:\n",
        "    return\n",
        "      \n",
        "client.run('Token')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}