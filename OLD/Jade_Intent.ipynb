{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jade_Intent.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "jO4z2rK1C44X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content\")\n",
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!wget https://launchpad.net/~alessandro-strada/+archive/ubuntu/ppa/+files/google-drive-ocamlfuse_0.7.0-0ubuntu1~ubuntu18.04.1_amd64.deb\n",
        "!apt-get install libfuse2\n",
        "!dpkg --install google-drive-ocamlfuse_0.7.0-0ubuntu1~ubuntu18.04.1_amd64.deb\n",
        "#!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "# Create a directory and mount Google Drive using that directory.\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "print('Files in Drive:')\n",
        "!ls drive/\n",
        "\n",
        "# Create a file in Drive.\n",
        "!echo \"This newly created file will appear in your Drive file list.\" > drive/created.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P0Mf89larrq0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install dialogflow\n",
        "!pip install discord-rewrite\n",
        "!git clone https://github.com/tensorflow/models.git\n",
        "!apt-get -qq install libprotobuf-java protobuf-compiler\n",
        "!protoc ./models/research/object_detection/protos/string_int_label_map.proto --python_out=.\n",
        "!cp -R models/research/object_detection/ object_detection/\n",
        "!rm -rf models\n",
        "!pip install dblpy\n",
        "!apt-get install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "!pip install plotly\n",
        "!wget http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat\n",
        "!git clone https://github.com/lengstrom/fast-style-transfer.git\n",
        "!cp -a fast-style-transfer/. ./\n",
        "!pip install moviepy\n",
        "!apt-get install ffmpeg\n",
        "!pip install psutil\n",
        "!pip install py-cpuinfo\n",
        "!pip install speedtest-cli\n",
        "!apt-get install cmake\n",
        "!pip install summa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v1_WHyKdrtyT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import dialogflow\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import discord\n",
        "import asyncio\n",
        "#from discord.ext.commands import Bot\n",
        "#from discord.ext import commands\n",
        "import dbl\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import resource\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Object Detection\n",
        "\n",
        "import scipy.misc\n",
        "import six.moves.urllib as urllib\n",
        "import tarfile\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "#import face_recognition\n",
        "import requests\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "!pip install --trusted-host pypi.python.org moviepy\n",
        "from subprocess import Popen, PIPE\n",
        "import subprocess\n",
        "\n",
        "#Summarize\n",
        "from summa import summarizer\n",
        "\n",
        "#OCR\n",
        "\n",
        "import cv2\n",
        "import pytesseract\n",
        "\n",
        "#Neural Style\n",
        "\n",
        "import scipy.io\n",
        "from functools import reduce\n",
        "\n",
        "import os.path\n",
        "if os.path.isfile(\"imagenet-vgg-verydeep-19.mat\")==False:\n",
        "  !wget http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat\n",
        "else:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DMns6fB9iJy5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Intent Stuff\n",
        "I... Intentionally put these here! yeah...\n",
        "pls don't kill me"
      ]
    },
    {
      "metadata": {
        "id": "b4Mxpv0ctMO3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!export GCLOUD_PROJECT=JADE\n",
        "import os\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"drive/USERBOT/JADE.json\"\n",
        "!export GOOGLE_APPLICATION_CREDENTIALS=\"drive/USERBOT/JADE.json\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dD45A8cZrjEN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def detect_intent_texts(project_id, session_id, texts, language_code):\n",
        "    \"\"\"Returns the result of detect intent with texts as inputs.\n",
        "\n",
        "    Using the same `session_id` between requests allows continuation\n",
        "    of the conversaion.\"\"\"\n",
        "    import dialogflow_v2 as dialogflow\n",
        "    session_client = dialogflow.SessionsClient()\n",
        "\n",
        "    session = session_client.session_path(project_id, session_id)\n",
        "    #print('Session path: {}\\n'.format(session))\n",
        "\n",
        "    text_input = dialogflow.types.TextInput(\n",
        "        text=texts, language_code=language_code)\n",
        "\n",
        "    query_input = dialogflow.types.QueryInput(text=text_input)\n",
        "    \n",
        "    response = session_client.detect_intent(session=session, query_input=query_input)\n",
        "\n",
        "    #print('=' * 20)\n",
        "    #print('Query text: {}'.format(response.query_result.query_text))\n",
        "    #print('Detected intent: {} (confidence: {})\\n'.format(\n",
        "    #      response.query_result.intent.display_name,\n",
        "    #      response.query_result.intent_detection_confidence))\n",
        "    #print('Fulfillment text: {}\\n'.format(\n",
        "    #      response.query_result.fulfillment_text))\n",
        "    #print(str(response.query_result.parameters))\n",
        "        \n",
        "    return str(response.query_result.intent.display_name), response.query_result.parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q36-is5RiXJh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Neural Style Definitions\n",
        "Woo colors!!"
      ]
    },
    {
      "metadata": {
        "id": "OgW8PpoKiWF_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = scipy.io.loadmat('imagenet-vgg-verydeep-19.mat')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T3BuDEUpidna",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _conv_layer(input, weights, bias):\n",
        "    conv = tf.nn.conv2d(input, tf.constant(weights), strides=(1, 1, 1, 1),\n",
        "            padding='SAME')\n",
        "    return tf.nn.bias_add(conv, bias)\n",
        "\n",
        "def _pool_layer(input):\n",
        "    return tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
        "            padding='SAME')\n",
        "\n",
        "def preprocess(image, mean_pixel):\n",
        "    return (image - mean_pixel).astype('float32')\n",
        "\n",
        "def unprocess(image, mean_pixel):\n",
        "    return (image + mean_pixel).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DoXB14_Sif3_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def netp(input_image):\n",
        "    layers = (\n",
        "        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
        "\n",
        "        'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
        "\n",
        "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n",
        "        'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
        "\n",
        "        'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n",
        "        'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
        "\n",
        "        'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n",
        "        'relu5_3', 'conv5_4', 'relu5_4'\n",
        "    )\n",
        "    weight = data['layers'][0]\n",
        "    net = {}\n",
        "    current = input_image\n",
        "    for i, name in enumerate(layers):\n",
        "        kind = name[:4]\n",
        "        if kind == 'conv':\n",
        "            kernels, bias = weight[i][0][0][0][0]\n",
        "            # matconvnet: weights are [width, height, in_channels, out_channels]\n",
        "            # tensorflow: weights are [height, width, in_channels, out_channels]\n",
        "            kernels = np.transpose(kernels, (1, 0, 2, 3))\n",
        "            bias = bias.reshape(-1)\n",
        "            current = _conv_layer(current, kernels, bias)\n",
        "        elif kind == 'relu':\n",
        "            current = tf.nn.relu(current)\n",
        "        elif kind == 'pool':\n",
        "            current = _pool_layer(current)\n",
        "        net[name] = current\n",
        "\n",
        "    assert len(net) == len(layers)\n",
        "    return net#, mean_pixel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CEpobGhKikWi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _tensor_size(tensor):\n",
        "    from operator import mul\n",
        "    return reduce(mul, (d.value for d in tensor.get_shape()), 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ol-To7cZil1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def imsave(path, img):\n",
        "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "    scipy.misc.imsave(path, img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "noZ1QShSrMhx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qoiBldxHq27q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "async def download_file(url, file_name, file_type):\n",
        "    if file_type == 'exe' or file_name == 'js':\n",
        "        return\n",
        "    headers = {\n",
        "    'User-agent': 'Mozilla/5.0 (Windows NT 6.3; rv:36.0) Gecko/20100101 Firefox/36.0'\n",
        "    }\n",
        "    r = requests.get(url, headers=headers, stream=True)\n",
        "    with open(\"images/\"+str(file_name)+'.'+str(file_type), 'wb') as f:\n",
        "    #with open(\"images/\"+str(file_name)+'.jpg', 'wb') as f:\n",
        "        for chunk in r.iter_content(chunk_size=1024):\n",
        "            if chunk:\n",
        "                f.write(chunk)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uxQTvf7RDsG3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Object Detection\n",
        "OOH A BANANNA *is looking at an apple*"
      ]
    },
    {
      "metadata": {
        "id": "frCUdGw-D3Q2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# What model to download.\n",
        "# MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
        "\n",
        "# model with more accurancy but up to you use a diferent model\n",
        "MODEL_NAME = 'faster_rcnn_inception_v2_coco_2018_01_28'\n",
        "\n",
        "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "\n",
        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
        "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = os.path.join('object_detection/data', 'mscoco_label_map.pbtxt')\n",
        "\n",
        "NUM_CLASSES = 90\n",
        "\n",
        "opener = urllib.request.URLopener()\n",
        "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "tar_file = tarfile.open(MODEL_FILE)\n",
        "for file in tar_file.getmembers():\n",
        "  file_name = os.path.basename(file.name)\n",
        "  if 'frozen_inference_graph.pb' in file_name:\n",
        "    tar_file.extract(file, os.getcwd())\n",
        "    \n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "  od_graph_def = tf.GraphDef()\n",
        "  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "    serialized_graph = fid.read()\n",
        "    od_graph_def.ParseFromString(serialized_graph)\n",
        "    tf.import_graph_def(od_graph_def, name='')\n",
        "    \n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "print(\"DONE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wXzCm9hiD33x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_image_into_numpy_array(image):\n",
        "  (im_width, im_height) = image.size\n",
        "  try:\n",
        "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
        "  except:\n",
        "    return np.array(image.getdata()).reshape((im_height, im_width, 4)).astype(np.uint8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C5_9AT0SD7Xn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For the sake of simplicity we will use only 2 images:\n",
        "# image1.jpg\n",
        "# image2.jpg\n",
        "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
        "PATH_TO_TEST_IMAGES_DIR = 'object_detection/test_images'\n",
        "TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 3) ]\n",
        "\n",
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (12, 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S4NRaDcZivgA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Chatbot stuff\n",
        "Becasue that - that is extrememly nessisary"
      ]
    },
    {
      "metadata": {
        "id": "qD5WZWfNUVnS",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import nltk\n",
        "import os\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "import codecs\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from collections import namedtuple\n",
        "from tensorflow.python.ops import lookup_ops\n",
        "\n",
        "import codecs\n",
        "import json\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class HParams:\n",
        "    def __init__(self, model_dir):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model_dir: Name of the folder storing the hparams.json file.\n",
        "        \"\"\"\n",
        "        self.hparams = self.load_hparams(model_dir)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_hparams(model_dir):\n",
        "        \"\"\"Load hparams from an existing directory.\"\"\"\n",
        "        hparams_file = os.path.join(model_dir, \"hparams.json\")\n",
        "        if tf.gfile.Exists(hparams_file):\n",
        "            print(\"# Loading hparams from {} ...\".format(hparams_file))\n",
        "            with codecs.getreader(\"utf-8\")(tf.gfile.GFile(hparams_file, \"rb\")) as f:\n",
        "                try:\n",
        "                    hparams_values = json.load(f)\n",
        "                    hparams = tf.contrib.training.HParams(**hparams_values)\n",
        "                except ValueError:\n",
        "                    print(\"Error loading hparams file.\")\n",
        "                    return None\n",
        "            return hparams\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "COMMENT_LINE_STT = \"#==\"\n",
        "CONVERSATION_SEP = \"===\"\n",
        "\n",
        "AUG0_FOLDER = \"Augment0\"\n",
        "AUG1_FOLDER = \"Augment1\"\n",
        "AUG2_FOLDER = \"Augment2\"\n",
        "\n",
        "MAX_LEN = 1000  # Assume no line in the training data is having more than this number of characters\n",
        "VOCAB_FILE = \"vocab.txt\"\n",
        "\n",
        "\n",
        "class TokenizedData:\n",
        "    def __init__(self, corpus_dir, hparams=None, training=True, buffer_size=8192):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            corpus_dir: Name of the folder storing corpus files for training.\n",
        "            hparams: The object containing the loaded hyper parameters. If None, it will be \n",
        "                    initialized here.\n",
        "            training: Whether to use this object for training.\n",
        "            buffer_size: The buffer size used for mapping process during data processing.\n",
        "        \"\"\"\n",
        "        if hparams is None:\n",
        "            self.hparams = HParams(corpus_dir).hparams\n",
        "        else:\n",
        "            self.hparams = hparams\n",
        "\n",
        "        self.src_max_len = self.hparams.src_max_len\n",
        "        self.tgt_max_len = self.hparams.tgt_max_len\n",
        "\n",
        "        self.training = training\n",
        "        self.text_set = None\n",
        "        self.id_set = None\n",
        "\n",
        "        vocab_file = os.path.join(corpus_dir, VOCAB_FILE)\n",
        "        self.vocab_size, _ = check_vocab(vocab_file)\n",
        "        self.vocab_table = lookup_ops.index_table_from_file(vocab_file,\n",
        "                                                            default_value=self.hparams.unk_id)\n",
        "        # print(\"vocab_size = {}\".format(self.vocab_size))\n",
        "\n",
        "        if training:\n",
        "            self.case_table = prepare_case_table()\n",
        "            self.reverse_vocab_table = None\n",
        "            self._load_corpus(corpus_dir)\n",
        "            self._convert_to_tokens(buffer_size)\n",
        "        else:\n",
        "            self.case_table = None\n",
        "            self.reverse_vocab_table = \\\n",
        "                lookup_ops.index_to_string_table_from_file(vocab_file,\n",
        "                                                           default_value=self.hparams.unk_token)\n",
        "\n",
        "    def get_training_batch(self, num_threads=4):\n",
        "        assert self.training\n",
        "\n",
        "        buffer_size = self.hparams.batch_size * 400\n",
        "\n",
        "        # Comment this line for debugging.\n",
        "        train_set = self.id_set.shuffle(buffer_size=buffer_size)\n",
        "\n",
        "        # Create a target input prefixed with BOS and a target output suffixed with EOS.\n",
        "        # After this mapping, each element in the train_set contains 3 columns/items.\n",
        "        train_set = train_set.map(lambda src, tgt:\n",
        "                                  (src, tf.concat(([self.hparams.bos_id], tgt), 0),\n",
        "                                   tf.concat((tgt, [self.hparams.eos_id]), 0)),\n",
        "                                  num_parallel_calls=num_threads).prefetch(buffer_size)\n",
        "\n",
        "        # Add in sequence lengths.\n",
        "        train_set = train_set.map(lambda src, tgt_in, tgt_out:\n",
        "                                  (src, tgt_in, tgt_out, tf.size(src), tf.size(tgt_in)),\n",
        "                                  num_parallel_calls=num_threads).prefetch(buffer_size)\n",
        "\n",
        "        def batching_func(x):\n",
        "            return x.padded_batch(\n",
        "                self.hparams.batch_size,\n",
        "                # The first three entries are the source and target line rows, these have unknown-length\n",
        "                # vectors. The last two entries are the source and target row sizes, which are scalars.\n",
        "                padded_shapes=(tf.TensorShape([None]),  # src\n",
        "                               tf.TensorShape([None]),  # tgt_input\n",
        "                               tf.TensorShape([None]),  # tgt_output\n",
        "                               tf.TensorShape([]),      # src_len\n",
        "                               tf.TensorShape([])),     # tgt_len\n",
        "                # Pad the source and target sequences with eos tokens. Though we don't generally need to\n",
        "                # do this since later on we will be masking out calculations past the true sequence.\n",
        "                padding_values=(self.hparams.eos_id,  # src\n",
        "                                self.hparams.eos_id,  # tgt_input\n",
        "                                self.hparams.eos_id,  # tgt_output\n",
        "                                0,       # src_len -- unused\n",
        "                                0))      # tgt_len -- unused\n",
        "\n",
        "        if self.hparams.num_buckets > 1:\n",
        "            bucket_width = (self.src_max_len + self.hparams.num_buckets - 1) // self.hparams.num_buckets\n",
        "\n",
        "            # Parameters match the columns in each element of the dataset.\n",
        "            def key_func(unused_1, unused_2, unused_3, src_len, tgt_len):\n",
        "                # Calculate bucket_width by maximum source sequence length. Pairs with length [0, bucket_width)\n",
        "                # go to bucket 0, length [bucket_width, 2 * bucket_width) go to bucket 1, etc. Pairs with\n",
        "                # length over ((num_bucket-1) * bucket_width) words all go into the last bucket.\n",
        "                # Bucket sentence pairs by the length of their source sentence and target sentence.\n",
        "                bucket_id = tf.maximum(src_len // bucket_width, tgt_len // bucket_width)\n",
        "                return tf.to_int64(tf.minimum(self.hparams.num_buckets, bucket_id))\n",
        "\n",
        "            # No key to filter the dataset. Therefore the key is unused.\n",
        "            def reduce_func(unused_key, windowed_data):\n",
        "                return batching_func(windowed_data)\n",
        "\n",
        "            batched_dataset = train_set.apply(\n",
        "                tf.contrib.data.group_by_window(key_func=key_func,\n",
        "                                                reduce_func=reduce_func,\n",
        "                                                window_size=self.hparams.batch_size))\n",
        "        else:\n",
        "            batched_dataset = batching_func(train_set)\n",
        "\n",
        "        batched_iter = batched_dataset.make_initializable_iterator()\n",
        "        (src_ids, tgt_input_ids, tgt_output_ids, src_seq_len, tgt_seq_len) = (batched_iter.get_next())\n",
        "\n",
        "        return BatchedInput(initializer=batched_iter.initializer,\n",
        "                            source=src_ids,\n",
        "                            target_input=tgt_input_ids,\n",
        "                            target_output=tgt_output_ids,\n",
        "                            source_sequence_length=src_seq_len,\n",
        "                            target_sequence_length=tgt_seq_len)\n",
        "\n",
        "    def get_inference_batch(self, src_dataset):\n",
        "        text_dataset = src_dataset.map(lambda src: tf.string_split([src]).values)\n",
        "\n",
        "        if self.hparams.src_max_len_infer:\n",
        "            text_dataset = text_dataset.map(lambda src: src[:self.hparams.src_max_len_infer])\n",
        "        # Convert the word strings to ids\n",
        "        id_dataset = text_dataset.map(lambda src: tf.cast(self.vocab_table.lookup(src),\n",
        "                                                          tf.int32))\n",
        "        if self.hparams.source_reverse:\n",
        "            id_dataset = id_dataset.map(lambda src: tf.reverse(src, axis=[0]))\n",
        "        # Add in the word counts.\n",
        "        id_dataset = id_dataset.map(lambda src: (src, tf.size(src)))\n",
        "\n",
        "        def batching_func(x):\n",
        "            return x.padded_batch(\n",
        "                self.hparams.batch_size_infer,\n",
        "                # The entry is the source line rows; this has unknown-length vectors.\n",
        "                # The last entry is the source row size; this is a scalar.\n",
        "                padded_shapes=(tf.TensorShape([None]),  # src\n",
        "                               tf.TensorShape([])),     # src_len\n",
        "                # Pad the source sequences with eos tokens. Though notice we don't generally need to\n",
        "                # do this since later on we will be masking out calculations past the true sequence.\n",
        "                padding_values=(self.hparams.eos_id,  # src\n",
        "                                0))                   # src_len -- unused\n",
        "\n",
        "        id_dataset = batching_func(id_dataset)\n",
        "\n",
        "        infer_iter = id_dataset.make_initializable_iterator()\n",
        "        (src_ids, src_seq_len) = infer_iter.get_next()\n",
        "\n",
        "        return BatchedInput(initializer=infer_iter.initializer,\n",
        "                            source=src_ids,\n",
        "                            target_input=None,\n",
        "                            target_output=None,\n",
        "                            source_sequence_length=src_seq_len,\n",
        "                            target_sequence_length=None)\n",
        "\n",
        "    def _load_corpus(self, corpus_dir):\n",
        "        for fd in range(2, -1, -1):\n",
        "            file_list = []\n",
        "            if fd == 0:\n",
        "                file_dir = os.path.join(corpus_dir, AUG0_FOLDER)\n",
        "            elif fd == 1:\n",
        "                file_dir = os.path.join(corpus_dir, AUG1_FOLDER)\n",
        "            else:\n",
        "                file_dir = os.path.join(corpus_dir, AUG2_FOLDER)\n",
        "\n",
        "            for data_file in sorted(os.listdir(file_dir)):\n",
        "                full_path_name = os.path.join(file_dir, data_file)\n",
        "                if os.path.isfile(full_path_name) and data_file.lower().endswith('.txt'):\n",
        "                    file_list.append(full_path_name)\n",
        "\n",
        "            assert len(file_list) > 0\n",
        "            dataset = tf.data.TextLineDataset(file_list)\n",
        "\n",
        "            src_dataset = dataset.filter(lambda line:\n",
        "                                         tf.logical_and(tf.size(line) > 0,\n",
        "                                                        tf.equal(tf.substr(line, 0, 2), tf.constant('Q:'))))\n",
        "            src_dataset = src_dataset.map(lambda line:\n",
        "                                          tf.substr(line, 2, MAX_LEN)).prefetch(4096)\n",
        "            tgt_dataset = dataset.filter(lambda line:\n",
        "                                         tf.logical_and(tf.size(line) > 0,\n",
        "                                                        tf.equal(tf.substr(line, 0, 2), tf.constant('A:'))))\n",
        "            tgt_dataset = tgt_dataset.map(lambda line:\n",
        "                                          tf.substr(line, 2, MAX_LEN)).prefetch(4096)\n",
        "\n",
        "            src_tgt_dataset = tf.data.Dataset.zip((src_dataset, tgt_dataset))\n",
        "            if fd == 1:\n",
        "                src_tgt_dataset = src_tgt_dataset.repeat(self.hparams.aug1_repeat_times)\n",
        "            elif fd == 2:\n",
        "                src_tgt_dataset = src_tgt_dataset.repeat(self.hparams.aug2_repeat_times)\n",
        "\n",
        "            if self.text_set is None:\n",
        "                self.text_set = src_tgt_dataset\n",
        "            else:\n",
        "                self.text_set = self.text_set.concatenate(src_tgt_dataset)\n",
        "\n",
        "    def _convert_to_tokens(self, buffer_size):\n",
        "        # The following 3 steps act as a python String lower() function\n",
        "        # Split to characters\n",
        "        self.text_set = self.text_set.map(lambda src, tgt:\n",
        "                                          (tf.string_split([src], delimiter='').values,\n",
        "                                           tf.string_split([tgt], delimiter='').values)\n",
        "                                          ).prefetch(buffer_size)\n",
        "        # Convert all upper case characters to lower case characters\n",
        "        self.text_set = self.text_set.map(lambda src, tgt:\n",
        "                                          (self.case_table.lookup(src), self.case_table.lookup(tgt))\n",
        "                                          ).prefetch(buffer_size)\n",
        "        # Join characters back to strings\n",
        "        self.text_set = self.text_set.map(lambda src, tgt:\n",
        "                                          (tf.reduce_join([src]), tf.reduce_join([tgt]))\n",
        "                                          ).prefetch(buffer_size)\n",
        "\n",
        "        # Split to word tokens\n",
        "        self.text_set = self.text_set.map(lambda src, tgt:\n",
        "                                          (tf.string_split([src]).values, tf.string_split([tgt]).values)\n",
        "                                          ).prefetch(buffer_size)\n",
        "        # Remove sentences longer than the model allows\n",
        "        self.text_set = self.text_set.map(lambda src, tgt:\n",
        "                                          (src[:self.src_max_len], tgt[:self.tgt_max_len])\n",
        "                                          ).prefetch(buffer_size)\n",
        "\n",
        "        # Reverse the source sentence if applicable\n",
        "        if self.hparams.source_reverse:\n",
        "            self.text_set = self.text_set.map(lambda src, tgt:\n",
        "                                              (tf.reverse(src, axis=[0]), tgt)\n",
        "                                              ).prefetch(buffer_size)\n",
        "\n",
        "        # Convert the word strings to ids.  Word strings that are not in the vocab get\n",
        "        # the lookup table's default_value integer.\n",
        "        self.id_set = self.text_set.map(lambda src, tgt:\n",
        "                                        (tf.cast(self.vocab_table.lookup(src), tf.int32),\n",
        "                                         tf.cast(self.vocab_table.lookup(tgt), tf.int32))\n",
        "                                        ).prefetch(buffer_size)\n",
        "\n",
        "\n",
        "def check_vocab(vocab_file):\n",
        "    \"\"\"Check to make sure vocab_file exists\"\"\"\n",
        "    if tf.gfile.Exists(vocab_file):\n",
        "        vocab_list = []\n",
        "        with codecs.getreader(\"utf-8\")(tf.gfile.GFile(vocab_file, \"rb\")) as f:\n",
        "            for word in f:\n",
        "                vocab_list.append(word.strip())\n",
        "    else:\n",
        "        raise ValueError(\"The vocab_file does not exist. Please run the script to create it.\")\n",
        "\n",
        "    return len(vocab_list), vocab_list\n",
        "\n",
        "\n",
        "def prepare_case_table():\n",
        "    keys = tf.constant([chr(i) for i in range(32, 127)])\n",
        "\n",
        "    l1 = [chr(i) for i in range(32, 65)]\n",
        "    l2 = [chr(i) for i in range(97, 123)]\n",
        "    l3 = [chr(i) for i in range(91, 127)]\n",
        "    values = tf.constant(l1 + l2 + l3)\n",
        "\n",
        "    return tf.contrib.lookup.HashTable(\n",
        "        tf.contrib.lookup.KeyValueTensorInitializer(keys, values), ' ')\n",
        "\n",
        "\n",
        "class BatchedInput(namedtuple(\"BatchedInput\",\n",
        "                              [\"initializer\",\n",
        "                               \"source\",\n",
        "                               \"target_input\",\n",
        "                               \"target_output\",\n",
        "                               \"source_sequence_length\",\n",
        "                               \"target_sequence_length\"])):\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def get_initializer(init_op, seed=None, init_weight=None):\n",
        "    \"\"\"Create an initializer. init_weight is only for uniform.\"\"\"\n",
        "    if init_op == \"uniform\":\n",
        "        assert init_weight\n",
        "        return tf.random_uniform_initializer(-init_weight, init_weight, seed=seed)\n",
        "    elif init_op == \"glorot_normal\":\n",
        "        return tf.contrib.keras.initializers.glorot_normal(seed=seed)\n",
        "    elif init_op == \"glorot_uniform\":\n",
        "        return tf.contrib.keras.initializers.glorot_uniform(seed=seed)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown init_op %s\" % init_op)\n",
        "\n",
        "\n",
        "# def get_device_str(device_id, num_gpus):\n",
        "#     \"\"\"Return a device string for multi-GPU setup.\"\"\"\n",
        "#     if num_gpus == 0:\n",
        "#         return \"/cpu:0\"\n",
        "#     device_str_output = \"/gpu:%d\" % (device_id % num_gpus)\n",
        "#     return device_str_output\n",
        "\n",
        "\n",
        "def create_embbeding(vocab_size, embed_size, dtype=tf.float32, scope=None):\n",
        "    \"\"\"Create embedding matrix for both encoder and decoder.\"\"\"\n",
        "    with tf.variable_scope(scope or \"embeddings\", dtype=dtype):\n",
        "        embedding = tf.get_variable(\"embedding\", [vocab_size, embed_size], dtype)\n",
        "\n",
        "    return embedding\n",
        "\n",
        "\n",
        "def _single_cell(num_units, keep_prob, device_str=None):\n",
        "    \"\"\"Create an instance of a single RNN cell.\"\"\"\n",
        "    single_cell = tf.contrib.rnn.GRUCell(num_units)\n",
        "\n",
        "    if keep_prob < 1.0:\n",
        "        single_cell = tf.contrib.rnn.DropoutWrapper(cell=single_cell, input_keep_prob=keep_prob)\n",
        "\n",
        "    # Device Wrapper\n",
        "    if device_str:\n",
        "        single_cell = tf.contrib.rnn.DeviceWrapper(single_cell, device_str)\n",
        "\n",
        "    return single_cell\n",
        "\n",
        "\n",
        "def create_rnn_cell(num_units, num_layers, keep_prob):\n",
        "    \"\"\"Create multi-layer RNN cell.\"\"\"\n",
        "    cell_list = []\n",
        "    for i in range(num_layers):\n",
        "        single_cell = _single_cell(num_units=num_units, keep_prob=keep_prob)\n",
        "        cell_list.append(single_cell)\n",
        "\n",
        "    if len(cell_list) == 1:  # Single layer.\n",
        "        return cell_list[0]\n",
        "    else:  # Multi layers\n",
        "        return tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "\n",
        "\n",
        "def gradient_clip(gradients, max_gradient_norm):\n",
        "    \"\"\"Clipping gradients of a model.\"\"\"\n",
        "    clipped_gradients, gradient_norm = tf.clip_by_global_norm(gradients, max_gradient_norm)\n",
        "    gradient_norm_summary = [tf.summary.scalar(\"grad_norm\", gradient_norm)]\n",
        "    gradient_norm_summary.append(\n",
        "        tf.summary.scalar(\"clipped_gradient\", tf.global_norm(clipped_gradients)))\n",
        "\n",
        "    return clipped_gradients, gradient_norm_summary\n",
        "\n",
        "from tensorflow.python.layers import core as layers_core\n",
        "\n",
        "\n",
        "class ModelCreator(object):\n",
        "    \"\"\"Sequence-to-sequence model creator to create models for training or inference\"\"\"\n",
        "    def __init__(self, training, tokenized_data, batch_input, scope=None):\n",
        "        \"\"\"\n",
        "        Create the model.\n",
        "\n",
        "        Args:\n",
        "            training: A boolean value to indicate whether this model will be used for training.\n",
        "            tokenized_data: The data object containing all information required for the model.\n",
        "            scope: scope of the model.\n",
        "        \"\"\"\n",
        "        self.training = training\n",
        "        self.batch_input = batch_input\n",
        "        self.vocab_table = tokenized_data.vocab_table\n",
        "        self.vocab_size = tokenized_data.vocab_size\n",
        "        self.reverse_vocab_table = tokenized_data.reverse_vocab_table\n",
        "\n",
        "        hparams = tokenized_data.hparams\n",
        "        self.hparams = hparams\n",
        "\n",
        "        self.num_layers = hparams.num_layers\n",
        "        self.time_major = hparams.time_major\n",
        "\n",
        "        # Initializer\n",
        "        initializer = get_initializer(\n",
        "            hparams.init_op, hparams.random_seed, hparams.init_weight)\n",
        "        tf.get_variable_scope().set_initializer(initializer)\n",
        "\n",
        "        # Embeddings\n",
        "        self.embedding = (create_embbeding(vocab_size=self.vocab_size,\n",
        "                                                        embed_size=hparams.num_units,\n",
        "                                                        scope=scope))\n",
        "        # This batch_size might vary among each batch instance due to the bucketing and/or reach\n",
        "        # the end of the training set. Treat it as size_of_the_batch.\n",
        "        self.batch_size = tf.size(self.batch_input.source_sequence_length)\n",
        "\n",
        "        # Projection\n",
        "        with tf.variable_scope(scope or \"build_network\"):\n",
        "            with tf.variable_scope(\"decoder/output_projection\"):\n",
        "                self.output_layer = layers_core.Dense(\n",
        "                    self.vocab_size, use_bias=False, name=\"output_projection\")\n",
        "\n",
        "        # Training or inference graph\n",
        "        print(\"# Building graph for the model ...\")\n",
        "        res = self.build_graph(hparams, scope=scope)\n",
        "\n",
        "        if training:\n",
        "            self.train_loss = res[1]\n",
        "            self.word_count = tf.reduce_sum(self.batch_input.source_sequence_length) + \\\n",
        "                              tf.reduce_sum(self.batch_input.target_sequence_length)\n",
        "            # Count the number of predicted words for compute perplexity.\n",
        "            self.predict_count = tf.reduce_sum(self.batch_input.target_sequence_length)\n",
        "        else:\n",
        "            self.infer_logits, _, self.final_context_state, self.sample_id = res\n",
        "            self.sample_words = self.reverse_vocab_table.lookup(tf.to_int64(self.sample_id))\n",
        "\n",
        "        self.global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "        params = tf.trainable_variables()\n",
        "\n",
        "        # Gradients update operation for training the model.\n",
        "        if training:\n",
        "            self.learning_rate = tf.placeholder(tf.float32, shape=[], name='learning_rate')\n",
        "            opt = tf.train.AdamOptimizer(self.learning_rate)\n",
        "\n",
        "            gradients = tf.gradients(self.train_loss, params)\n",
        "\n",
        "            clipped_gradients, gradient_norm_summary = gradient_clip(\n",
        "                gradients, max_gradient_norm=hparams.max_gradient_norm)\n",
        "\n",
        "            self.update = opt.apply_gradients(\n",
        "                zip(clipped_gradients, params), global_step=self.global_step)\n",
        "\n",
        "            # Summary\n",
        "            self.train_summary = tf.summary.merge([\n",
        "                tf.summary.scalar(\"learning_rate\", self.learning_rate),\n",
        "                tf.summary.scalar(\"train_loss\", self.train_loss),\n",
        "            ] + gradient_norm_summary)\n",
        "        else:\n",
        "            self.infer_summary = tf.no_op()\n",
        "\n",
        "        # Saver\n",
        "        self.saver = tf.train.Saver(tf.global_variables())\n",
        "\n",
        "        # Print trainable variables\n",
        "        if training:\n",
        "            print(\"# Trainable variables:\")\n",
        "            for param in params:\n",
        "                print(\"  {}, {}, {}\".format(param.name, str(param.get_shape()), param.op.device))\n",
        "\n",
        "    def train_step(self, sess, learning_rate):\n",
        "        \"\"\"Run one step of training.\"\"\"\n",
        "        assert self.training\n",
        "\n",
        "        return sess.run([self.update,\n",
        "                         self.train_loss,\n",
        "                         self.predict_count,\n",
        "                         self.train_summary,\n",
        "                         self.global_step,\n",
        "                         self.word_count,\n",
        "                         self.batch_size],\n",
        "                        feed_dict={self.learning_rate: learning_rate})\n",
        "\n",
        "    def build_graph(self, hparams, scope=None):\n",
        "        \"\"\"Creates a sequence-to-sequence model with dynamic RNN decoder API.\"\"\"\n",
        "        dtype = tf.float32\n",
        "\n",
        "        with tf.variable_scope(scope or \"dynamic_seq2seq\", dtype=dtype):\n",
        "            # Encoder\n",
        "            encoder_outputs, encoder_state = self._build_encoder(hparams)\n",
        "\n",
        "            # Decoder\n",
        "            logits, sample_id, final_context_state = self._build_decoder(\n",
        "                encoder_outputs, encoder_state, hparams)\n",
        "\n",
        "            # Loss\n",
        "            if self.training:\n",
        "                loss = self._compute_loss(logits)\n",
        "            else:\n",
        "                loss = None\n",
        "\n",
        "            return logits, loss, final_context_state, sample_id\n",
        "\n",
        "    def _build_encoder(self, hparams):\n",
        "        \"\"\"Build an encoder.\"\"\"\n",
        "        source = self.batch_input.source\n",
        "        if self.time_major:\n",
        "            source = tf.transpose(source)\n",
        "\n",
        "        with tf.variable_scope(\"encoder\") as scope:\n",
        "            dtype = scope.dtype\n",
        "            # Look up embedding, emp_inp: [max_time, batch_size, num_units]\n",
        "            encoder_emb_inp = tf.nn.embedding_lookup(self.embedding, source)\n",
        "\n",
        "            # Encoder_outpus: [max_time, batch_size, num_units]\n",
        "            cell = self._build_encoder_cell(hparams)\n",
        "\n",
        "            encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
        "                cell,\n",
        "                encoder_emb_inp,\n",
        "                dtype=dtype,\n",
        "                sequence_length=self.batch_input.source_sequence_length,\n",
        "                time_major=self.time_major)\n",
        "\n",
        "        return encoder_outputs, encoder_state\n",
        "\n",
        "    def _build_encoder_cell(self, hparams):\n",
        "        \"\"\"Build a multi-layer RNN cell that can be used by encoder.\"\"\"\n",
        "        return create_rnn_cell(\n",
        "            num_units=hparams.num_units,\n",
        "            num_layers=hparams.num_layers,\n",
        "            keep_prob=hparams.keep_prob)\n",
        "\n",
        "    def _build_decoder(self, encoder_outputs, encoder_state, hparams):\n",
        "        \"\"\"Build and run a RNN decoder with a final projection layer.\"\"\"\n",
        "        bos_id = tf.cast(self.vocab_table.lookup(tf.constant(hparams.bos_token)), tf.int32)\n",
        "        eos_id = tf.cast(self.vocab_table.lookup(tf.constant(hparams.eos_token)), tf.int32)\n",
        "\n",
        "        # maximum_iteration: The maximum decoding steps.\n",
        "        if hparams.tgt_max_len_infer:\n",
        "            maximum_iterations = hparams.tgt_max_len_infer\n",
        "        else:\n",
        "            decoding_length_factor = 2.0\n",
        "            max_encoder_length = tf.reduce_max(self.batch_input.source_sequence_length)\n",
        "            maximum_iterations = tf.to_int32(tf.round(\n",
        "                tf.to_float(max_encoder_length) * decoding_length_factor))\n",
        "\n",
        "        # Decoder.\n",
        "        with tf.variable_scope(\"decoder\") as decoder_scope:\n",
        "            cell, decoder_initial_state = self._build_decoder_cell(\n",
        "                hparams, encoder_outputs, encoder_state,\n",
        "                self.batch_input.source_sequence_length)\n",
        "\n",
        "            # Training\n",
        "            if self.training:\n",
        "                # decoder_emp_inp: [max_time, batch_size, num_units]\n",
        "                target_input = self.batch_input.target_input\n",
        "                if self.time_major:\n",
        "                    target_input = tf.transpose(target_input)\n",
        "                decoder_emb_inp = tf.nn.embedding_lookup(self.embedding, target_input)\n",
        "\n",
        "                # Helper\n",
        "                helper = tf.contrib.seq2seq.TrainingHelper(\n",
        "                    decoder_emb_inp, self.batch_input.target_sequence_length,\n",
        "                    time_major=self.time_major)\n",
        "\n",
        "                # Decoder\n",
        "                my_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "                    cell,\n",
        "                    helper,\n",
        "                    decoder_initial_state,)\n",
        "\n",
        "                # Dynamic decoding\n",
        "                outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                    my_decoder,\n",
        "                    output_time_major=self.time_major,\n",
        "                    swap_memory=True,\n",
        "                    scope=decoder_scope)\n",
        "\n",
        "                sample_id = outputs.sample_id\n",
        "                logits = self.output_layer(outputs.rnn_output)\n",
        "            # Inference\n",
        "            else:\n",
        "                beam_width = hparams.beam_width\n",
        "                length_penalty_weight = hparams.length_penalty_weight\n",
        "                start_tokens = tf.fill([self.batch_size], bos_id)\n",
        "                end_token = eos_id\n",
        "\n",
        "                if beam_width > 0:\n",
        "                    my_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
        "                        cell=cell,\n",
        "                        embedding=self.embedding,\n",
        "                        start_tokens=start_tokens,\n",
        "                        end_token=end_token,\n",
        "                        initial_state=decoder_initial_state,\n",
        "                        beam_width=beam_width,\n",
        "                        output_layer=self.output_layer,\n",
        "                        length_penalty_weight=length_penalty_weight)\n",
        "                else:\n",
        "                    # Helper\n",
        "                    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
        "                        self.embedding, start_tokens, end_token)\n",
        "\n",
        "                    # Decoder\n",
        "                    my_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "                        cell,\n",
        "                        helper,\n",
        "                        decoder_initial_state,\n",
        "                        output_layer=self.output_layer  # applied per timestep\n",
        "                    )\n",
        "\n",
        "                # Dynamic decoding\n",
        "                outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                    my_decoder,\n",
        "                    maximum_iterations=maximum_iterations,\n",
        "                    output_time_major=self.time_major,\n",
        "                    swap_memory=True,\n",
        "                    scope=decoder_scope)\n",
        "\n",
        "                if beam_width > 0:\n",
        "                    logits = tf.no_op()\n",
        "                    sample_id = outputs.predicted_ids\n",
        "                else:\n",
        "                    logits = outputs.rnn_output\n",
        "                    sample_id = outputs.sample_id\n",
        "\n",
        "        return logits, sample_id, final_context_state\n",
        "\n",
        "    def _build_decoder_cell(self, hparams, encoder_outputs, encoder_state,\n",
        "                            source_sequence_length):\n",
        "        \"\"\"Build a RNN cell with attention mechanism that can be used by decoder.\"\"\"\n",
        "        num_units = hparams.num_units\n",
        "        num_layers = hparams.num_layers\n",
        "        beam_width = hparams.beam_width\n",
        "\n",
        "        dtype = tf.float32\n",
        "\n",
        "        if self.time_major:\n",
        "            memory = tf.transpose(encoder_outputs, [1, 0, 2])\n",
        "        else:\n",
        "            memory = encoder_outputs\n",
        "\n",
        "        if not self.training and beam_width > 0:\n",
        "            memory = tf.contrib.seq2seq.tile_batch(memory, multiplier=beam_width)\n",
        "            source_sequence_length = tf.contrib.seq2seq.tile_batch(source_sequence_length,\n",
        "                                                                   multiplier=beam_width)\n",
        "            encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state,\n",
        "                                                          multiplier=beam_width)\n",
        "            batch_size = self.batch_size * beam_width\n",
        "        else:\n",
        "            batch_size = self.batch_size\n",
        "\n",
        "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "            num_units, memory, memory_sequence_length=source_sequence_length)\n",
        "\n",
        "        cell = create_rnn_cell(\n",
        "            num_units=num_units,\n",
        "            num_layers=num_layers,\n",
        "            keep_prob=hparams.keep_prob)\n",
        "\n",
        "        # Only generate alignment in greedy INFER mode.\n",
        "        alignment_history = (not self.training and beam_width == 0)\n",
        "        cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "            cell,\n",
        "            attention_mechanism,\n",
        "            attention_layer_size=num_units,\n",
        "            alignment_history=alignment_history,\n",
        "            name=\"attention\")\n",
        "\n",
        "        if hparams.pass_hidden_state:\n",
        "            decoder_initial_state = cell.zero_state(batch_size, dtype).clone(cell_state=encoder_state)\n",
        "        else:\n",
        "            decoder_initial_state = cell.zero_state(batch_size, dtype)\n",
        "\n",
        "        return cell, decoder_initial_state\n",
        "\n",
        "    def _compute_loss(self, logits):\n",
        "        \"\"\"Compute optimization loss.\"\"\"\n",
        "        target_output = self.batch_input.target_output\n",
        "        if self.time_major:\n",
        "            target_output = tf.transpose(target_output)\n",
        "        max_time = self.get_max_time(target_output)\n",
        "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            labels=target_output, logits=logits)\n",
        "        target_weights = tf.sequence_mask(\n",
        "            self.batch_input.target_sequence_length, max_time, dtype=logits.dtype)\n",
        "        if self.time_major:\n",
        "            target_weights = tf.transpose(target_weights)\n",
        "\n",
        "        loss = tf.reduce_sum(crossent * target_weights) / tf.to_float(self.batch_size)\n",
        "        return loss\n",
        "\n",
        "    def get_max_time(self, tensor):\n",
        "        time_axis = 0 if self.time_major else 1\n",
        "        return tensor.shape[time_axis].value or tf.shape(tensor)[time_axis]\n",
        "\n",
        "    def infer(self, sess):\n",
        "        assert not self.training\n",
        "        _, infer_summary, _, sample_words = sess.run([\n",
        "            self.infer_logits, self.infer_summary, self.sample_id, self.sample_words\n",
        "        ])\n",
        "\n",
        "        # make sure outputs is of shape [batch_size, time]\n",
        "        if self.time_major:\n",
        "            sample_words = sample_words.transpose()\n",
        "\n",
        "        return sample_words, infer_summary\n",
        "      \n",
        "import os\n",
        "\n",
        "UPPER_FILE = \"upper_words.txt\"\n",
        "STORIES_FILE = \"stories.txt\"\n",
        "JOKES_FILE = \"jokes.txt\"\n",
        "\n",
        "\n",
        "class KnowledgeBase:\n",
        "    def __init__(self):\n",
        "        self.upper_words = {}\n",
        "        self.stories = {}\n",
        "        self.jokes = []\n",
        "\n",
        "    def load_knbase(self, knbase_dir):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "             knbase_dir: Name of the KnowledgeBase folder. The file names inside are fixed.\n",
        "        \"\"\"\n",
        "        upper_file_name = os.path.join(knbase_dir, UPPER_FILE)\n",
        "        stories_file_name = os.path.join(knbase_dir, STORIES_FILE)\n",
        "        jokes_file_name = os.path.join(knbase_dir, JOKES_FILE)\n",
        "\n",
        "        with open(upper_file_name, 'r') as upper_f:\n",
        "            for line in upper_f:\n",
        "                ln = line.strip()\n",
        "                if not ln or ln.startswith('#'):\n",
        "                    continue\n",
        "                cap_words = ln.split(',')\n",
        "                for cpw in cap_words:\n",
        "                    tmp = cpw.strip()\n",
        "                    self.upper_words[tmp.lower()] = tmp\n",
        "\n",
        "        with open(stories_file_name, 'r') as stories_f:\n",
        "            s_name, s_content = '', ''\n",
        "            for line in stories_f:\n",
        "                ln = line.strip()\n",
        "                if not ln or ln.startswith('#'):\n",
        "                    continue\n",
        "                if ln.startswith('_NAME:'):\n",
        "                    if s_name != '' and s_content != '':\n",
        "                        self.stories[s_name] = s_content\n",
        "                        s_name, s_content = '', ''\n",
        "                    s_name = ln[6:].strip().lower()\n",
        "                elif ln.startswith('_CONTENT:'):\n",
        "                    s_content = ln[9:].strip()\n",
        "                else:\n",
        "                    s_content += ' ' + ln.strip()\n",
        "\n",
        "            if s_name != '' and s_content != '':  # The last one\n",
        "                self.stories[s_name] = s_content\n",
        "\n",
        "        with open(jokes_file_name, 'r') as jokes_f:\n",
        "            for line in jokes_f:\n",
        "                ln = line.strip()\n",
        "                if not ln or ln.startswith('#'):\n",
        "                    continue\n",
        "                self.jokes.append(ln)\n",
        "                \n",
        "class SessionData:\n",
        "    def __init__(self):\n",
        "        self.session_dict = {}\n",
        "\n",
        "    def add_session(self):\n",
        "        items = self.session_dict.items()\n",
        "        if items:\n",
        "            last_id = max(k for k, v in items)\n",
        "        else:\n",
        "            last_id = 0\n",
        "        new_id = last_id + 1\n",
        "\n",
        "        self.session_dict[new_id] = ChatSession(new_id)\n",
        "        return new_id\n",
        "\n",
        "    def get_session(self, session_id):\n",
        "        return self.session_dict[session_id]\n",
        "\n",
        "\n",
        "class ChatSession:\n",
        "    def __init__(self, session_id):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            session_id: The integer ID of the chat session.\n",
        "        \"\"\"\n",
        "        self.session_id = session_id\n",
        "\n",
        "        self.howru_asked = False\n",
        "\n",
        "        self.user_name = None\n",
        "        self.call_me = None\n",
        "\n",
        "        self.last_question = None\n",
        "        self.last_answer = None\n",
        "        self.update_pair = True\n",
        "\n",
        "        self.last_topic = None\n",
        "        self.keep_topic = False\n",
        "\n",
        "        # Will be storing the information of the pending action:\n",
        "        # The action function name, the parameter for answer yes, and the parameter for answer no.\n",
        "        self.pending_action = {'func': None, 'Yes': None, 'No': None}\n",
        "\n",
        "    def before_prediction(self):\n",
        "        self.update_pair = True\n",
        "        self.keep_topic = False\n",
        "\n",
        "    def after_prediction(self, new_question, new_answer):\n",
        "        self._update_last_pair(new_question, new_answer)\n",
        "        self._clear_last_topic()\n",
        "\n",
        "    def _update_last_pair(self, new_question, new_answer):\n",
        "        \"\"\"\n",
        "        Last pair is updated after each prediction except in a few cases.\n",
        "        \"\"\"\n",
        "        if self.update_pair:\n",
        "            self.last_question = new_question\n",
        "            self.last_answer = new_answer\n",
        "\n",
        "    def _clear_last_topic(self):\n",
        "        \"\"\"\n",
        "        Last topic is cleared after each prediction except in a few cases.\n",
        "        \"\"\"\n",
        "        if not self.keep_topic:\n",
        "            self.last_topic = None\n",
        "\n",
        "    def update_pending_action(self, func_name, yes_para, no_para):\n",
        "        self.pending_action['func'] = func_name\n",
        "        self.pending_action['Yes'] = yes_para\n",
        "        self.pending_action['No'] = no_para\n",
        "\n",
        "    def clear_pending_action(self):\n",
        "        \"\"\"\n",
        "        Pending action is, and only is, cleared at the end of function: execute_pending_action_and_reply.\n",
        "        \"\"\"\n",
        "        self.pending_action['func'] = None\n",
        "        self.pending_action['Yes'] = None\n",
        "        self.pending_action['No'] = None\n",
        "        \n",
        "import re\n",
        "\n",
        "\n",
        "def check_patterns_and_replace(question):\n",
        "    pat_matched, new_sentence, para_list = _check_arithmetic_pattern_and_replace(question)\n",
        "\n",
        "    if not pat_matched:\n",
        "        pat_matched, new_sentence, para_list = _check_not_username_pattern_and_replace(new_sentence)\n",
        "\n",
        "    if not pat_matched:\n",
        "        pat_matched, new_sentence, para_list = _check_username_callme_pattern_and_replace(new_sentence)\n",
        "\n",
        "    return pat_matched, new_sentence, para_list\n",
        "\n",
        "\n",
        "def _check_arithmetic_pattern_and_replace(sentence):\n",
        "    pat_matched, ind_list, num_list = _contains_arithmetic_pattern(sentence)\n",
        "    if pat_matched:\n",
        "        s1, e1 = ind_list[0]\n",
        "        s2, e2 = ind_list[1]\n",
        "        # Leave spaces around the special tokens so that NLTK knows they are separate tokens\n",
        "        new_sentence = sentence[:s1] + ' _num1_ ' + sentence[e1:s2] + ' _num2_ ' + sentence[e2:]\n",
        "        return True, new_sentence, num_list\n",
        "    else:\n",
        "        return False, sentence, num_list\n",
        "\n",
        "\n",
        "def _contains_arithmetic_pattern(sentence):\n",
        "    numbers = [\n",
        "        \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\",\n",
        "        \"eight\", \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\",\n",
        "        \"fifteen\", \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\",\n",
        "        \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\",\n",
        "        \"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
        "\n",
        "    pat_op1 = re.compile(\n",
        "        r'\\s(plus|add|added|\\+|minus|subtract|subtracted|-|times|multiply|multiplied|\\*|divide|(divided\\s+by)|/)\\s',\n",
        "        re.IGNORECASE)\n",
        "    pat_op2 = re.compile(r'\\s((sum\\s+of)|(product\\s+of))\\s', re.IGNORECASE)\n",
        "    pat_as = re.compile(r'((\\bis\\b)|=|(\\bequals\\b)|(\\bget\\b))', re.IGNORECASE)\n",
        "\n",
        "    mat_op1 = re.search(pat_op1, sentence)\n",
        "    mat_op2 = re.search(pat_op2, sentence)\n",
        "    mat_as = re.search(pat_as, sentence)\n",
        "    if (mat_op1 or mat_op2) and mat_as:  # contains an arithmetic operator and an assign operator\n",
        "        # Replace all occurrences of word \"and\" with 3 whitespaces before feeding to\n",
        "        # the pattern matcher.\n",
        "        pat_and = re.compile(r'\\band\\b', re.IGNORECASE)\n",
        "        if mat_op1:\n",
        "            tmp_sentence = pat_and.sub('   ', sentence)\n",
        "        else:  # Do not support word 'and' in the English numbers any more as that can be ambiguous.\n",
        "            tmp_sentence = pat_and.sub('_T_', sentence)\n",
        "\n",
        "        number_rx = r'(?:{})'.format('|'.join(numbers))\n",
        "        pat_num = re.compile(r'\\b{0}(?:(?:\\s+(?:and\\s+)?|-){0})*\\b|\\d+'.format(number_rx),\n",
        "                             re.IGNORECASE)\n",
        "        ind_list = [(m.start(0), m.end(0)) for m in re.finditer(pat_num, tmp_sentence)]\n",
        "        num_list = []\n",
        "        if len(ind_list) == 2:  # contains exactly two numbers\n",
        "            for start, end in ind_list:\n",
        "                text = sentence[start:end]\n",
        "                text_int = _text2int(text)\n",
        "                if text_int == -1:\n",
        "                    return False, [], []\n",
        "                num_list.append(text_int)\n",
        "\n",
        "            return True, ind_list, num_list\n",
        "\n",
        "    return False, [], []\n",
        "\n",
        "\n",
        "def _text2int(text):\n",
        "    if text.isdigit():\n",
        "        return int(text)\n",
        "\n",
        "    num_words = {}\n",
        "    units = [\n",
        "        \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
        "        \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
        "        \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\",\n",
        "    ]\n",
        "    tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]\n",
        "    scales = [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
        "\n",
        "    num_words[\"and\"] = (1, 0)\n",
        "    for idx, word in enumerate(units):\n",
        "        num_words[word] = (1, idx)\n",
        "    for idx, word in enumerate(tens):\n",
        "        num_words[word] = (1, idx * 10)\n",
        "    for idx, word in enumerate(scales):\n",
        "        num_words[word] = (10 ** (idx * 3 or 2), 0)\n",
        "\n",
        "    current = result = 0\n",
        "    for word in text.replace(\"-\", \" \").lower().split():\n",
        "        if word not in num_words:\n",
        "            return -1\n",
        "\n",
        "        scale, increment = num_words[word]\n",
        "        current = current * scale + increment\n",
        "        if scale > 100:\n",
        "            result += current\n",
        "            current = 0\n",
        "\n",
        "    return result + current\n",
        "\n",
        "\n",
        "def _check_not_username_pattern_and_replace(sentence):\n",
        "    import nltk\n",
        "\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    tmp_sentence = ' '.join(tokens[:]).strip()\n",
        "\n",
        "    pat_not_but = re.compile(r'(\\s|^)my\\s+name\\s+is\\s+(not|n\\'t)\\s+(.+?)(\\s\\.|\\s,|\\s!)\\s*but\\s+(.+?)(\\s\\.|\\s,|\\s!|$)',\n",
        "                             re.IGNORECASE)\n",
        "    mat_not_but = re.search(pat_not_but, tmp_sentence)\n",
        "\n",
        "    pat_not = re.compile(r'(\\s|^)my\\s+name\\s+is\\s+(not|n\\'t)\\s+(.+?)(\\s\\.|\\s,|\\s!|$)', re.IGNORECASE)\n",
        "    mat_not = re.search(pat_not, tmp_sentence)\n",
        "\n",
        "    para_list = []\n",
        "    found = 0\n",
        "    if mat_not_but:\n",
        "        wrong_name = mat_not_but.group(3).strip()\n",
        "        correct_name = mat_not_but.group(5).strip()\n",
        "        para_list.append(correct_name)\n",
        "        new_sentence = sentence.replace(wrong_name, ' _ignored_ ', 1).replace(correct_name, ' _name_ ', 1)\n",
        "        # print(\"User name is not: {}, but {}.\".format(wrong_name, correct_name))\n",
        "        found += 1\n",
        "    elif mat_not:\n",
        "        wrong_name = mat_not.group(3).strip()\n",
        "        new_sentence = sentence.replace(wrong_name, ' _ignored_ ', 1)\n",
        "        # print(\"User name is not: {}.\".format(wrong_name))\n",
        "        found += 1\n",
        "    else:\n",
        "        new_sentence = sentence\n",
        "        # print(\"Wrong name not found.\")\n",
        "\n",
        "    if found >= 1:\n",
        "        return True, new_sentence, para_list\n",
        "    else:\n",
        "        return False, sentence, para_list\n",
        "\n",
        "\n",
        "def _check_username_callme_pattern_and_replace(sentence):\n",
        "    import nltk\n",
        "\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    tmp_sentence = ' '.join(tokens[:]).strip()\n",
        "\n",
        "    pat_name = re.compile(r'(\\s|^)my\\s+name\\s+is\\s+(.+?)(\\s\\.|\\s,|\\s!|$)', re.IGNORECASE)\n",
        "    pat_call = re.compile(r'(\\s|^)call\\s+me\\s+(.+?)(\\s(please|pls))?(\\s\\.|\\s,|\\s!|$)', re.IGNORECASE)\n",
        "\n",
        "    mat_name = re.search(pat_name, tmp_sentence)\n",
        "    mat_call = re.search(pat_call, tmp_sentence)\n",
        "\n",
        "    para_list = []\n",
        "    found = 0\n",
        "    if mat_name:\n",
        "        user_name = mat_name.group(2).strip()\n",
        "        para_list.append(user_name)\n",
        "        new_sentence = sentence.replace(user_name, ' _name_ ', 1)\n",
        "        # print(\"User name is: {}.\".format(user_name))\n",
        "        found += 1\n",
        "    else:\n",
        "        para_list.append('')  # reserve the slot\n",
        "        new_sentence = sentence\n",
        "        # print(\"User name not found.\")\n",
        "\n",
        "    if mat_call:\n",
        "        call_me = mat_call.group(2).strip()\n",
        "        para_list.append(call_me)\n",
        "        new_sentence = new_sentence.replace(call_me, ' _callme_ ')\n",
        "        # print(\"Call me {}.\".format(call_me))\n",
        "        found += 1\n",
        "    else:\n",
        "        para_list.append('')\n",
        "        # print(\"call me not found.\")\n",
        "\n",
        "    if found >= 1:\n",
        "        return True, new_sentence, para_list\n",
        "    else:\n",
        "        return False, sentence, para_list\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sentence = \"My name is jack brown. Please call me Mr. Brown.\"\n",
        "    print(\"# {}\".format(sentence))\n",
        "    _, ns, _ = _check_username_callme_pattern_and_replace(sentence)\n",
        "    print(ns)\n",
        "\n",
        "    sentence = \"My name is Bo Shao.\"\n",
        "    print(\"# {}\".format(sentence))\n",
        "    _, ns, _ = _check_username_callme_pattern_and_replace(sentence)\n",
        "    print(ns)\n",
        "\n",
        "    sentence = \"You can call me Dr. Shao.\"\n",
        "    print(\"# {}\".format(sentence))\n",
        "    _, ns, _ = _check_username_callme_pattern_and_replace(sentence)\n",
        "    print(ns)\n",
        "\n",
        "    sentence = \"Call me Ms. Tailor please.\"\n",
        "    print(\"# {}\".format(sentence))\n",
        "    _, ns, _ = _check_username_callme_pattern_and_replace(sentence)\n",
        "    print(ns)\n",
        "\n",
        "    sentence = \"My name is Mark. Please call me Mark D.\"\n",
        "    print(\"# {}\".format(sentence))\n",
        "    _, ns, _ = _check_username_callme_pattern_and_replace(sentence)\n",
        "    print(ns)\n",
        "\n",
        "    sentence = \"My name is not just Shao, but Bo Shao.\"\n",
        "    print(\"# {}\".format(sentence))\n",
        "    _, ns, _ = _check_not_username_pattern_and_replace(sentence)\n",
        "    print(ns)\n",
        "\n",
        "    sentence = \"My name is not just Shao.\"\n",
        "    print(\"# {}\".format(sentence))\n",
        "    _, ns, _ = _check_not_username_pattern_and_replace(sentence)\n",
        "    print(ns)\n",
        "    \n",
        "import calendar as cal\n",
        "import datetime as dt\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "\n",
        "\n",
        "class FunctionData:\n",
        "    easy_list = [\n",
        "        \"\", \"\",\n",
        "        \"Here you are: \",\n",
        "        \"Here is the result: \",\n",
        "        \"That's easy: \",\n",
        "        \"That was an easy one: \",\n",
        "        \"It was a piece of cake: \",\n",
        "        \"That's simple, and I know how to solve it: \",\n",
        "        \"That wasn't hard. Here is the result: \",\n",
        "        \"Oh, I know how to deal with this: \"\n",
        "    ]\n",
        "    hard_list = [\n",
        "        \"\", \"\",\n",
        "        \"Here you are: \",\n",
        "        \"Here is the result: \",\n",
        "        \"That's a little hard: \",\n",
        "        \"That was an tough one, and I had to use a calculator: \",\n",
        "        \"That's a little difficult, but I know how to solve it: \",\n",
        "        \"It was hard and took me a little while to figure it out. Here is the result: \",\n",
        "        \"It took me a little while, and finally I got the result: \",\n",
        "        \"I had to use my cell phone for this calculation. Here is the outcome: \"\n",
        "    ]\n",
        "    ask_howru_list = [\n",
        "        \"And you?\",\n",
        "        \"How are you?\",\n",
        "        \"How about yourself?\"\n",
        "    ]\n",
        "    ask_name_list = [\n",
        "        \"May I also have your name, please?\",\n",
        "        \"Would you also like to tell me your name, please?\",\n",
        "        \"And, how should I call you, please?\",\n",
        "        \"And, what do you want me to call you, dear sir or madam?\"\n",
        "    ]\n",
        "\n",
        "    def __init__(self, knowledge_base, chat_session, html_format):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            knowledge_base: The knowledge base data needed for prediction.\n",
        "            chat_session: The chat session object that can be read and written.\n",
        "            html_format: Whether out_sentence is in HTML format.\n",
        "        \"\"\"\n",
        "        self.knowledge_base = knowledge_base\n",
        "        self.chat_session = chat_session\n",
        "        self.html_format = html_format\n",
        "\n",
        "    \"\"\"\n",
        "    # Rule 2: Date and Time\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def get_date_time():\n",
        "        return time.strftime(\"%Y-%m-%d %H:%M\")\n",
        "\n",
        "    @staticmethod\n",
        "    def get_time():\n",
        "        return time.strftime(\"%I:%M %p\")\n",
        "\n",
        "    @staticmethod\n",
        "    def get_today():\n",
        "        return \"{:%B %d, %Y}\".format(dt.date.today())\n",
        "\n",
        "    @staticmethod\n",
        "    def get_weekday(day_delta):\n",
        "        now = dt.datetime.now()\n",
        "        if day_delta == 'd_2':\n",
        "            day_time = now - dt.timedelta(days=2)\n",
        "        elif day_delta == 'd_1':\n",
        "            day_time = now - dt.timedelta(days=1)\n",
        "        elif day_delta == 'd1':\n",
        "            day_time = now + dt.timedelta(days=1)\n",
        "        elif day_delta == 'd2':\n",
        "            day_time = now + dt.timedelta(days=2)\n",
        "        else:\n",
        "            day_time = now\n",
        "\n",
        "        weekday = cal.day_name[day_time.weekday()]\n",
        "        return \"{}, {:%B %d, %Y}\".format(weekday, day_time)\n",
        "\n",
        "    \"\"\"\n",
        "    # Rule 3: Stories and Jokes, and last topic\n",
        "    \"\"\"\n",
        "    def get_story_any(self):\n",
        "        self.chat_session.last_topic = \"STORY\"\n",
        "        self.chat_session.keep_topic = True\n",
        "\n",
        "        stories = self.knowledge_base.stories\n",
        "        _, content = random.choice(list(stories.items()))\n",
        "        if not self.html_format:\n",
        "            content = re.sub(r'_np_', '', content)\n",
        "        return content\n",
        "\n",
        "    def get_story_name(self, story_name):\n",
        "        self.chat_session.last_topic = \"STORY\"\n",
        "        self.chat_session.keep_topic = True\n",
        "\n",
        "        stories = self.knowledge_base.stories\n",
        "        content = stories[story_name]\n",
        "        if not self.html_format:\n",
        "            content = re.sub(r'_np_', '', content)\n",
        "        return content\n",
        "\n",
        "    def get_joke_any(self):\n",
        "        self.chat_session.last_topic = \"JOKE\"\n",
        "        self.chat_session.keep_topic = True\n",
        "\n",
        "        jokes = self.knowledge_base.jokes\n",
        "        content = random.choice(jokes)\n",
        "        if not self.html_format:\n",
        "            content = re.sub(r'_np_', '', content)\n",
        "        return content\n",
        "\n",
        "    def continue_last_topic(self):\n",
        "        if self.chat_session.last_topic == \"STORY\":\n",
        "            self.chat_session.keep_topic = True\n",
        "            return self.get_story_any()\n",
        "        elif self.chat_session.last_topic == \"JOKE\":\n",
        "            self.chat_session.keep_topic = True\n",
        "            return self.get_joke_any()\n",
        "        else:\n",
        "            return \"Sorry, but what topic do you prefer?\"\n",
        "\n",
        "    \"\"\"\n",
        "    # Rule 4: Arithmetic ops\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def get_number_plus(num1, num2):\n",
        "        res = num1 + num2\n",
        "        desc = random.choice(FunctionData.easy_list)\n",
        "        return \"{}{} + {} = {}\".format(desc, num1, num2, res)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_number_minus(num1, num2):\n",
        "        res = num1 - num2\n",
        "        desc = random.choice(FunctionData.easy_list)\n",
        "        return \"{}{} - {} = {}\".format(desc, num1, num2, res)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_number_multiply(num1, num2):\n",
        "        res = num1 * num2\n",
        "        if num1 > 100 and num2 > 100 and num1 % 2 == 1 and num2 % 2 == 1:\n",
        "            desc = random.choice(FunctionData.hard_list)\n",
        "        else:\n",
        "            desc = random.choice(FunctionData.easy_list)\n",
        "        return \"{}{} * {} = {}\".format(desc, num1, num2, res)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_number_divide(num1, num2):\n",
        "        if num2 == 0:\n",
        "            return \"Sorry, but that does not make sense as the divisor cannot be zero.\"\n",
        "        else:\n",
        "            res = num1 / num2\n",
        "            if isinstance(res, int):\n",
        "                if 50 < num1 != num2 > 50:\n",
        "                    desc = random.choice(FunctionData.hard_list)\n",
        "                else:\n",
        "                    desc = random.choice(FunctionData.easy_list)\n",
        "                return \"{}{} / {} = {}\".format(desc, num1, num2, res)\n",
        "            else:\n",
        "                if num1 > 20 and num2 > 20:\n",
        "                    desc = random.choice(FunctionData.hard_list)\n",
        "                else:\n",
        "                    desc = random.choice(FunctionData.easy_list)\n",
        "                return \"{}{} / {} = {:.2f}\".format(desc, num1, num2, res)\n",
        "\n",
        "    \"\"\"\n",
        "    # Rule 5: User name, call me information, and last question and answer\n",
        "    \"\"\"\n",
        "    def ask_howru_if_not_yet(self):\n",
        "        howru_asked = self.chat_session.howru_asked\n",
        "        if howru_asked:\n",
        "            return \"\"\n",
        "        else:\n",
        "            self.chat_session.howru_asked = True\n",
        "            return random.choice(FunctionData.ask_howru_list)\n",
        "\n",
        "    def ask_name_if_not_yet(self):\n",
        "        user_name = self.chat_session.user_name\n",
        "        call_me = self.chat_session.call_me\n",
        "        if user_name or call_me:\n",
        "            return \"\"\n",
        "        else:\n",
        "            return random.choice(FunctionData.ask_name_list)\n",
        "\n",
        "    def get_user_name_and_reply(self):\n",
        "        user_name = self.chat_session.user_name\n",
        "        if user_name and user_name.strip() != '':\n",
        "            return user_name\n",
        "        else:\n",
        "            return \"Did you tell me your name? Sorry, I missed that.\"\n",
        "\n",
        "    def get_callme(self, punc_type):\n",
        "        call_me = self.chat_session.call_me\n",
        "        user_name = self.chat_session.user_name\n",
        "\n",
        "        if call_me and call_me.strip() != '':\n",
        "            if punc_type == 'comma0':\n",
        "                return \", {}\".format(call_me)\n",
        "            else:\n",
        "                return call_me\n",
        "        elif user_name and user_name.strip() != '':\n",
        "            if punc_type == 'comma0':\n",
        "                return \", {}\".format(user_name)\n",
        "            else:\n",
        "                return user_name\n",
        "        else:\n",
        "            return \"\"\n",
        "\n",
        "    def get_last_question(self):\n",
        "        # Do not record this pair as the last question and answer\n",
        "        self.chat_session.update_pair = False\n",
        "\n",
        "        last_question = self.chat_session.last_question\n",
        "        if last_question is None or last_question.strip() == '':\n",
        "            return \"You did not say anything.\"\n",
        "        else:\n",
        "            return \"You have just said: {}\".format(last_question)\n",
        "\n",
        "    def get_last_answer(self):\n",
        "        # Do not record this pair as the last question and answer\n",
        "        self.chat_session.update_pair = False\n",
        "\n",
        "        last_answer = self.chat_session.last_answer\n",
        "        if last_answer is None or last_answer.strip() == '':\n",
        "            return \"I did not say anything.\"\n",
        "        else:\n",
        "            return \"I have just said: {}\".format(last_answer)\n",
        "\n",
        "    def update_user_name(self, new_name):\n",
        "        return self.update_user_name_and_call_me(new_name=new_name)\n",
        "\n",
        "    def update_call_me(self, new_call):\n",
        "        return self.update_user_name_and_call_me(new_call=new_call)\n",
        "\n",
        "    def update_user_name_and_call_me(self, new_name=None, new_call=None):\n",
        "        user_name = self.chat_session.user_name\n",
        "        call_me = self.chat_session.call_me\n",
        "        # print(\"{}; {}; {}; {}\".format(user_name, call_me, new_name, new_call))\n",
        "\n",
        "        if user_name and new_name and new_name.strip() != '':\n",
        "            if new_name.lower() != user_name.lower():\n",
        "                self.chat_session.update_pending_action('update_user_name_confirmed', None, new_name)\n",
        "                return \"I am confused. I have your name as {}. Did I get it correctly?\".format(user_name)\n",
        "            else:\n",
        "                return \"You told me your name already. Thank you, {}, for assuring me.\".format(user_name)\n",
        "\n",
        "        if call_me and new_call and new_call.strip() != '':\n",
        "            if new_call.lower() != call_me.lower():\n",
        "                self.chat_session.update_pending_action('update_call_me_confirmed', new_call, None)\n",
        "                return \"You wanted me to call you {}. Would you like me to call you {} now?\"\\\n",
        "                    .format(call_me, new_call)\n",
        "            else:\n",
        "                return \"Thank you for letting me again, {}.\".format(call_me)\n",
        "\n",
        "        if new_call and new_call.strip() != '':\n",
        "            if new_name and new_name.strip() != '':\n",
        "                self.chat_session.user_name = new_name\n",
        "\n",
        "            self.chat_session.call_me = new_call\n",
        "            return \"Thank you, {}.\".format(new_call)\n",
        "        elif new_name and new_name.strip() != '':\n",
        "            self.chat_session.user_name = new_name\n",
        "            return \"Thank you, {}.\".format(new_name)\n",
        "\n",
        "        return \"Sorry, I am confused. I could not figure out what you meant.\"\n",
        "\n",
        "    def update_user_name_enforced(self, new_name):\n",
        "        if new_name and new_name.strip() != '':\n",
        "            self.chat_session.user_name = new_name\n",
        "            return \"OK, thank you, {}.\".format(new_name)\n",
        "        else:\n",
        "            self.chat_session.user_name = None  # Clear the existing user_name, if any.\n",
        "            return \"Sorry, I am lost.\"\n",
        "\n",
        "    def update_call_me_enforced(self, new_call):\n",
        "        if new_call and new_call.strip() != '':\n",
        "            self.chat_session.call_me = new_call\n",
        "            return \"OK, got it. Thank you, {}.\".format(new_call)\n",
        "        else:\n",
        "            self.chat_session.call_me = None  # Clear the existing call_me, if any.\n",
        "            return \"Sorry, I am totally lost.\"\n",
        "\n",
        "    def update_user_name_and_reply_papaya(self, new_name):\n",
        "        user_name = self.chat_session.user_name\n",
        "\n",
        "        if new_name and new_name.strip() != '':\n",
        "            if user_name:\n",
        "                if new_name.lower() != user_name.lower():\n",
        "                    self.chat_session.update_pending_action('update_user_name_confirmed', None, new_name)\n",
        "                    return \"I am confused. I have your name as {}. Did I get it correctly?\".format(user_name)\n",
        "                else:\n",
        "                    return \"Thank you, {}, for assuring me your name. My name is Papaya.\".format(user_name)\n",
        "            else:\n",
        "                self.chat_session.user_name = new_name\n",
        "                return \"Thank you, {}. BTW, my name is Papaya.\".format(new_name)\n",
        "        else:\n",
        "            return \"My name is Papaya. Thanks.\"\n",
        "\n",
        "    def correct_user_name(self, new_name):\n",
        "        if new_name and new_name.strip() != '':\n",
        "            self.chat_session.user_name = new_name\n",
        "            return \"Thank you, {}.\".format(new_name)\n",
        "        else:\n",
        "            # Clear the existing user_name and call_me information\n",
        "            self.chat_session.user_name = None\n",
        "            self.chat_session.call_me = None\n",
        "            return \"I am totally lost.\"\n",
        "\n",
        "    def clear_user_name_and_call_me(self):\n",
        "        self.chat_session.user_name = None\n",
        "        self.chat_session.call_me = None\n",
        "\n",
        "    def execute_pending_action_and_reply(self, answer):\n",
        "        func = self.chat_session.pending_action['func']\n",
        "        if func == 'update_user_name_confirmed':\n",
        "            if answer.lower() == 'yes':\n",
        "                reply = \"Thank you, {}, for confirming this.\".format(self.chat_session.user_name)\n",
        "            else:\n",
        "                new_name = self.chat_session.pending_action['No']\n",
        "                self.chat_session.user_name = new_name\n",
        "                reply = \"Thank you, {}, for correcting me.\".format(new_name)\n",
        "        elif func == 'update_call_me_confirmed':\n",
        "            if answer.lower() == 'yes':\n",
        "                new_call = self.chat_session.pending_action['Yes']\n",
        "                self.chat_session.call_me = new_call\n",
        "                reply = \"Thank you, {}, for correcting me.\".format(new_call)\n",
        "            else:\n",
        "                reply = \"Thank you. I will continue to call you {}.\".format(self.chat_session.call_me)\n",
        "        else:\n",
        "            reply = \"OK, thanks.\"  # Just presents a reply that is good for most situations\n",
        "\n",
        "        # Clear the pending action anyway\n",
        "        self.chat_session.clear_pending_action()\n",
        "        return reply\n",
        "\n",
        "    \"\"\"\n",
        "    # Other Rules: Client Code\n",
        "    \"\"\"\n",
        "    def client_code_show_picture_randomly(self, picture_name):\n",
        "        if not self.html_format:  # Ignored in the command line interface\n",
        "            return ''\n",
        "        else:\n",
        "            return ' _cc_start_show_picture_randomly_para1_' + picture_name + '_cc_end_'\n",
        "\n",
        "\n",
        "def call_function(func_info, knowledge_base=None, chat_session=None, para_list=None,\n",
        "                  html_format=False):\n",
        "    func_data = FunctionData(knowledge_base, chat_session, html_format=html_format)\n",
        "\n",
        "    func_dict = {\n",
        "        'get_date_time': FunctionData.get_date_time,\n",
        "        'get_time': FunctionData.get_time,\n",
        "        'get_today': FunctionData.get_today,\n",
        "        'get_weekday': FunctionData.get_weekday,\n",
        "\n",
        "        'get_story_any': func_data.get_story_any,\n",
        "        'get_story_name': func_data.get_story_name,\n",
        "        'get_joke_any': func_data.get_joke_any,\n",
        "        'continue_last_topic': func_data.continue_last_topic,\n",
        "\n",
        "        'get_number_plus': FunctionData.get_number_plus,\n",
        "        'get_number_minus': FunctionData.get_number_minus,\n",
        "        'get_number_multiply': FunctionData.get_number_multiply,\n",
        "        'get_number_divide': FunctionData.get_number_divide,\n",
        "\n",
        "        'ask_howru_if_not_yet': func_data.ask_howru_if_not_yet,\n",
        "        'ask_name_if_not_yet': func_data.ask_name_if_not_yet,\n",
        "        'get_user_name_and_reply': func_data.get_user_name_and_reply,\n",
        "        'get_callme': func_data.get_callme,\n",
        "        'get_last_question': func_data.get_last_question,\n",
        "        'get_last_answer': func_data.get_last_answer,\n",
        "\n",
        "        'update_user_name': func_data.update_user_name,\n",
        "        'update_call_me': func_data.update_call_me,\n",
        "        'update_user_name_and_call_me': func_data.update_user_name_and_call_me,\n",
        "        'update_user_name_enforced': func_data.update_user_name_enforced,\n",
        "        'update_call_me_enforced': func_data.update_call_me_enforced,\n",
        "        'update_user_name_and_reply_papaya': func_data.update_user_name_and_reply_papaya,\n",
        "\n",
        "        'correct_user_name': func_data.correct_user_name,\n",
        "        'clear_user_name_and_call_me': func_data.clear_user_name_and_call_me,\n",
        "\n",
        "        'execute_pending_action_and_reply': func_data.execute_pending_action_and_reply,\n",
        "\n",
        "        'client_code_show_picture_randomly': func_data.client_code_show_picture_randomly\n",
        "    }\n",
        "\n",
        "    para1_index = func_info.find('_para1_')\n",
        "    para2_index = func_info.find('_para2_')\n",
        "    if para1_index == -1:  # No parameter at all\n",
        "        func_name = func_info\n",
        "        if func_name in func_dict:\n",
        "            return func_dict[func_name]()\n",
        "    else:\n",
        "        func_name = func_info[:para1_index]\n",
        "        if para2_index == -1:  # Only one parameter\n",
        "            func_para = func_info[para1_index+7:]\n",
        "            if func_para == '_name_' and para_list is not None and len(para_list) >= 1:\n",
        "                return func_dict[func_name](para_list[0])\n",
        "            elif func_para == '_callme_' and para_list is not None and len(para_list) >= 2:\n",
        "                return func_dict[func_name](para_list[1])\n",
        "            else:  # The parameter value was embedded in the text (part of the string) of the training example.\n",
        "                return func_dict[func_name](func_para)\n",
        "        else:\n",
        "            func_para1 = func_info[para1_index+7:para2_index]\n",
        "            func_para2 = func_info[para2_index+7:]\n",
        "            if para_list is not None and len(para_list) >= 2:\n",
        "                para1_val = para_list[0]\n",
        "                para2_val = para_list[1]\n",
        "\n",
        "                if func_para1 == '_num1_' and func_para2 == '_num2_':\n",
        "                    return func_dict[func_name](para1_val, para2_val)\n",
        "                elif func_para1 == '_num2_' and func_para2 == '_num1_':\n",
        "                    return func_dict[func_name](para2_val, para1_val)\n",
        "                elif func_para1 == '_name_' and func_para2 == '_callme_':\n",
        "                    return func_dict[func_name](para1_val, para2_val)\n",
        "\n",
        "    return \"You beat me to it, and I cannot tell which is which for this question.\"\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "\n",
        "class BotPredictor(object):\n",
        "    def __init__(self, session, corpus_dir, knbase_dir, result_dir, result_file):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            session: The TensorFlow session.\n",
        "            corpus_dir: Name of the folder storing corpus files and vocab information.\n",
        "            knbase_dir: Name of the folder storing data files for the knowledge base.\n",
        "            result_dir: The folder containing the trained result files.\n",
        "            result_file: The file name of the trained model.\n",
        "        \"\"\"\n",
        "        self.session = session\n",
        "\n",
        "        # Prepare data and hyper parameters\n",
        "        print(\"# Prepare dataset placeholder and hyper parameters ...\")\n",
        "        tokenized_data = TokenizedData(corpus_dir=corpus_dir, training=False)\n",
        "\n",
        "        self.knowledge_base = KnowledgeBase()\n",
        "        self.knowledge_base.load_knbase(knbase_dir)\n",
        "\n",
        "        self.session_data = SessionData()\n",
        "\n",
        "        self.hparams = tokenized_data.hparams\n",
        "        self.src_placeholder = tf.placeholder(shape=[None], dtype=tf.string)\n",
        "        src_dataset = tf.data.Dataset.from_tensor_slices(self.src_placeholder)\n",
        "        self.infer_batch = tokenized_data.get_inference_batch(src_dataset)\n",
        "\n",
        "        # Create model\n",
        "        print(\"# Creating inference model ...\")\n",
        "        self.model = ModelCreator(training=False, tokenized_data=tokenized_data,\n",
        "                                  batch_input=self.infer_batch)\n",
        "        # Restore model weights\n",
        "        print(\"# Restoring model weights ...\")\n",
        "        self.model.saver.restore(session, os.path.join(result_dir, result_file))\n",
        "\n",
        "        self.session.run(tf.tables_initializer())\n",
        "\n",
        "    def predict(self, session_id, question, html_format=False):\n",
        "        chat_session = self.session_data.get_session(session_id)\n",
        "        chat_session.before_prediction()  # Reset before each prediction\n",
        "\n",
        "        if question.strip() == '':\n",
        "            answer = \"Don't you want to say something to me?\"\n",
        "            chat_session.after_prediction(question, answer)\n",
        "            return answer\n",
        "\n",
        "        pat_matched, new_sentence, para_list = check_patterns_and_replace(question)\n",
        "\n",
        "        for pre_time in range(2):\n",
        "            tokens = nltk.word_tokenize(new_sentence.lower())\n",
        "            tmp_sentence = [' '.join(tokens[:]).strip()]\n",
        "\n",
        "            self.session.run(self.infer_batch.initializer,\n",
        "                             feed_dict={self.src_placeholder: tmp_sentence})\n",
        "\n",
        "            outputs, _ = self.model.infer(self.session)\n",
        "\n",
        "            if self.hparams.beam_width > 0:\n",
        "                outputs = outputs[0]\n",
        "\n",
        "            eos_token = self.hparams.eos_token.encode(\"utf-8\")\n",
        "            outputs = outputs.tolist()[0]\n",
        "\n",
        "            if eos_token in outputs:\n",
        "                outputs = outputs[:outputs.index(eos_token)]\n",
        "\n",
        "            if pat_matched and pre_time == 0:\n",
        "                out_sentence, if_func_val = self._get_final_output(outputs, chat_session,\n",
        "                                                                   para_list=para_list,\n",
        "                                                                   html_format=html_format)\n",
        "                if if_func_val:\n",
        "                    chat_session.after_prediction(question, out_sentence)\n",
        "                    return out_sentence\n",
        "                else:\n",
        "                    new_sentence = question\n",
        "            else:\n",
        "                out_sentence, _ = self._get_final_output(outputs, chat_session,\n",
        "                                                         html_format=html_format)\n",
        "                chat_session.after_prediction(question, out_sentence)\n",
        "                return out_sentence\n",
        "\n",
        "    def _get_final_output(self, sentence, chat_session, para_list=None, html_format=False):\n",
        "        sentence = b' '.join(sentence).decode('utf-8')\n",
        "        if sentence == '':\n",
        "            return \"I don't know what to say.\", False\n",
        "\n",
        "        if_func_val = False\n",
        "        last_word = None\n",
        "        word_list = []\n",
        "        for word in sentence.split(' '):\n",
        "            word = word.strip()\n",
        "            if not word:\n",
        "                continue\n",
        "\n",
        "            if word.startswith('_func_val_'):\n",
        "                if_func_val = True\n",
        "                word = call_function(word[10:], knowledge_base=self.knowledge_base,\n",
        "                                     chat_session=chat_session, para_list=para_list,\n",
        "                                     html_format=html_format)\n",
        "                if word is None or word == '':\n",
        "                    continue\n",
        "            else:\n",
        "                if word in self.knowledge_base.upper_words:\n",
        "                    word = self.knowledge_base.upper_words[word]\n",
        "\n",
        "                if (last_word is None or last_word in ['.', '!', '?']) and not word[0].isupper():\n",
        "                    word = word.capitalize()\n",
        "\n",
        "            if not word.startswith('\\'') and word != 'n\\'t' \\\n",
        "                and (word[0] not in string.punctuation or word in ['(', '[', '{', '``', '$']) \\\n",
        "                and last_word not in ['(', '[', '{', '``', '$']:\n",
        "                word = ' ' + word\n",
        "\n",
        "            word_list.append(word)\n",
        "            last_word = word\n",
        "\n",
        "        return ''.join(word_list).strip(), if_func_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4-t1GV60UkpQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "PROJECT_ROOT = \"drive/AgateV2\"\n",
        "\n",
        "tf.reset_default_graph() \n",
        "\n",
        "global predictor\n",
        "predictor = BotPredictor(tf.Session(), corpus_dir=os.path.join(PROJECT_ROOT, 'Data', 'Corpus'), knbase_dir=os.path.join(PROJECT_ROOT, 'Data', 'KnowledgeBase'), result_dir=os.path.join(PROJECT_ROOT, 'Data', 'Result'), result_file='basic')\n",
        "global session_id \n",
        "session_id = predictor.session_data.add_session()\n",
        "\n",
        "def callBot(sentence):\n",
        "    global predictor\n",
        "    return predictor.predict(session_id, sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iJHrozrYUmEw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loop = asyncio.new_event_loop()\n",
        "asyncio.set_event_loop(asyncio.new_event_loop())\n",
        "#client = Bot(description=\"JADE AI\", command_prefix=\"\", pm_help = False)\n",
        "\n",
        "import aiohttp\n",
        "\n",
        "global url, dbltoken , headers\n",
        "\n",
        "client = discord.AutoShardedClient()\n",
        "\n",
        "@client.event\n",
        "async def on_ready():\n",
        "    global url,dbltoken,headers\n",
        "    dbltoken = \"NANI\"\n",
        "    url = \"https://discordbots.org/api/bots/\" + str(client.user.id) + \"/stats\"\n",
        "    headers = {\"Authorization\" : dbltoken}\n",
        "    \n",
        "    print('Logged in as '+client.user.name+' (ID:'+str(client.user.id)+') | Connected to '+str(len(client.guilds))+' servers | Connected to '+ str(len(set(client.get_all_members()))) +' users')\n",
        "    print('--------')\n",
        "    print('You are running Jade AI v0.4') #Do not change this. This will really help us support you, if you need support.\n",
        "    print('--------')\n",
        "    print(\"Discord.py verison: \" + discord.__version__)\n",
        "    print('--------')\n",
        "    print(str(len(client.shards))+\" shard(s)\")\n",
        "    \n",
        "    payload = {\"server_count\"  : len(client.guilds),\n",
        "              'shard_count': len(client.shards)}\n",
        "    async with aiohttp.ClientSession() as aioclient:\n",
        "            await aioclient.post(url, data=payload, headers=headers)\n",
        "\n",
        "@client.event        \n",
        "async def on_server_join(server):\n",
        "    global url,dbltoken,headers\n",
        "    payload = {\"server_count\"  : len(client.guilds)}\n",
        "    async with aiohttp.ClientSession() as aioclient:\n",
        "            await aioclient.post(url, data=payload, headers=headers)\n",
        "\n",
        "@client.event        \n",
        "async def on_server_remove(server):\n",
        "    global url,dbltoken,headers\n",
        "    payload = {\"server_count\"  : len(client.guilds)}\n",
        "    async with aiohttp.ClientSession() as aioclient:\n",
        "            await aioclient.post(url, data=payload, headers=headers)\n",
        "\n",
        "@client.event        \n",
        "async def on_message(message):\n",
        "    Mentioned=False\n",
        "    DM = False\n",
        "    \n",
        "    for user in message.mentions: \n",
        "      if user.id == \"421368639753551873\":\n",
        "        Mentioned = True\n",
        "        \n",
        "    if str(message.guild) == \"None\":\n",
        "      DM=True\n",
        "      \n",
        "    if not message.author.bot and not message.author.id == 410253782828449802 and ((message.content.startswith('JD ') or message.content.startswith('jd ') or Mentioned == True) or DM == True):\n",
        "      if DM == False:\n",
        "        ModMessage = message.content[3:]\n",
        "      else:\n",
        "         ModMessage = message.content\n",
        "      #print(ModMessage)\n",
        "      intent, params = detect_intent_texts(\"jade-204720\", \"j-i\", ModMessage, \"en\")\n",
        "    \n",
        "      if intent ==  \"Ping\":\n",
        "        print(\"\")\n",
        "        embed=discord.Embed(title=\"Poing!\", color=0x00a86b)\n",
        "        embed.set_author(name=\"Jade\", url=\"https://discordbots.org/bot/410253782828449802\", icon_url=\"https://preview.ibb.co/jVTDWL/jd-chibi-big.png\")\n",
        "        ltcn=0\n",
        "        for (x,y) in client.latencies:\n",
        "          if x==message.guild.shard_id:\n",
        "            ltcn = ltcn+y\n",
        "            \n",
        "        ltc=\"\"\n",
        "        ltc=ltcn/len(client.latencies)\n",
        "        embed.add_field(name=\"Ping\", value=\"Discord: \" + str(client.latency*1000) + \"ms\\nShard Avg: \" + str(ltc*1000) + \"ms\", inline=False)\n",
        "        embed.set_footer(text=\"Requested by: \" + str(message.author))\n",
        "        await message.channel.send(embed=embed)\n",
        "        \n",
        "      elif intent == \"Help\":\n",
        "        print(\"\")\n",
        "        embed=discord.Embed(title=\"Help\", color=0x00a86b)\n",
        "        embed.set_author(name=\"Jade\", url=\"https://discordbots.org/bot/410253782828449802\", icon_url=\"https://preview.ibb.co/jVTDWL/jd-chibi-big.png\")\n",
        "        embed.set_thumbnail(url=\"https://preview.ibb.co/cP9bkf/gif-2.gif\")\n",
        "        ltc=\"\"\n",
        "        for (x,y) in client.latencies:\n",
        "          if x==message.guild.shard_id:\n",
        "            ltc = y\n",
        "        embed.add_field(name=\"Attached to shard\", value=\"ID: \" + str(message.guild.shard_id) + \" of \" + str(len(client.shards)) + \"\\nShard Latency: \" + str(ltc*1000) + \"ms\", inline=False)\n",
        "        embed.add_field(name=\"Example prompts:\", value=\"\".join([\"Can you style this using un? [Attach Image] → Styled Image\",\n",
        "                                                                \"\\nHi, Jade! → Chat\", \"\\nCan I get some help? → Help\", \n",
        "                                                                \"\\nWhat's in this image? → Object Recognition\", \n",
        "                                                                \"\\nPing!!!!!!!!! → Pong\"]), inline=True)\n",
        "        embed.add_field(name=\"For more detailed help,\", value=\"go to [Jade's Github](https://github.com/JEF1056/JadeAI)\", inline=True)\n",
        "        embed.set_footer(text=\"Requested by: \" + str(message.author))\n",
        "        await message.channel.send(embed=embed)\n",
        "    \n",
        "      elif intent == \"Chatbot\":\n",
        "        msg = await message.channel.send(\"...\")\n",
        "        async with message.channel.typing():\n",
        "          print('\\n' + str(message.guild) + '\\n' + str(message.author) + \": \" + ModMessage)\n",
        "          ModMessage = ModMessage.replace(\"Jade\", \"Papaya\")\n",
        "          answer = callBot(ModMessage)\n",
        "          answer = answer.replace(\"Papaya\", \"Jade\")\n",
        "          answer = answer.replace(\"father\", \"daughter\")\n",
        "          answer = answer.replace(\"male\", \"female\")\n",
        "          answer = answer.replace(\"boy\", \"girl\")\n",
        "          answer = answer.replace(\"Although being a robot, I look like a normal 9 year old boy.\", \"I look pretty good ;D\")\n",
        "          await msg.edit(content=answer)\n",
        "          await message.channel.send(\"~\", delete_after=0.001)\n",
        "          print(\"Jade: \" + answer)\n",
        "          \n",
        "      elif intent == \"Image Recognition\":\n",
        "        embed=discord.Embed(title=\"Neural Style Issues:\", color=0x00a86b)\n",
        "        embed.set_author(name=\"Jade\", url=\"https://discordbots.org/bot/410253782828449802\", icon_url=\"https://preview.ibb.co/jVTDWL/jd-chibi-big.png\")\n",
        "        embed.set_thumbnail(url=\"https://preview.ibb.co/cP9bkf/gif-2.gif\")\n",
        "        \n",
        "        t1 = time.perf_counter()\n",
        "      \n",
        "        msg = await message.channel.send(\"...\")\n",
        "        NoRender=False\n",
        "        issue_count=0\n",
        "        file_dl = False\n",
        "      \n",
        "        try:\n",
        "          if \".mp4\" in message.attachments[0].url:\n",
        "            await download_file(message.attachments[0].url,'custom_img_1',\"mp4\")\n",
        "            video = True\n",
        "          else:\n",
        "            await download_file(message.attachments[0].url,'custom_img_1',\"jpg\")\n",
        "          print('\\n' + str(message.guild) + '\\n' + str(message.author) + \": \" + str(message.attachments[0].url))\n",
        "          file_dl=True\n",
        "        except Exception as e:\n",
        "          NoRender = True\n",
        "          embed.add_field(name=\"Issue #\" + str(issue_count), value=\"Cannot download file from link\" \"\\nOr unable to find an attachment\", inline=False)\n",
        "          issue_count = issue_count+1\n",
        "          #await message.channel.send(e)\n",
        "          exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "          fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
        "          print(exc_type, fname, exc_tb.tb_lineno)\n",
        "            \n",
        "        try:\n",
        "          split_str=str(params[\"url\"].values).split('\"')\n",
        "          link = split_str[1]\n",
        "          if \".mp4\" in link:\n",
        "            await download_file(link ,'custom_img_1',\"mp4\")\n",
        "          else:\n",
        "            await download_file(link ,'custom_img_1',\"jpg\")\n",
        "          print('\\n' + str(message.guild) + '\\n' + str(message.author) + \": \" + str(link))\n",
        "          NoRender = False\n",
        "        except Exception as e:\n",
        "          if file_dl==False:\n",
        "            NoRender = True\n",
        "            embed.add_field(name=\"Issue #\" + str(issue_count), value=\"Cannot download file from link\", inline=False)\n",
        "            issue_count = issue_count+1\n",
        "            #await message.channel.send(e)\n",
        "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
        "            print(exc_type, fname, exc_tb.tb_lineno)\n",
        "            \n",
        "        if NoRender == True:\n",
        "          await message.channel.send(embed=embed, delete_after=15)\n",
        "          async with message.channel.typing():\n",
        "            print('\\n' + str(message.guild) + '\\n' + str(message.author) + \": \" + ModMessage)\n",
        "            ModMessage = ModMessage.replace(\"Jade\", \"Papaya\")\n",
        "            answer = callBot(ModMessage)\n",
        "            answer = answer.replace(\"Papaya\", \"Jade\")\n",
        "            answer = answer.replace(\"father\", \"daughter\")\n",
        "            answer = answer.replace(\"male\", \"female\")\n",
        "            answer = answer.replace(\"boy\", \"girl\")\n",
        "            answer = answer.replace(\"Although being a robot, I look like a normal 9 year old boy.\", \"I look pretty good ;D\")\n",
        "            await msg.edit(content=answer)\n",
        "            print(\"Jade: \" + answer)\n",
        "            await message.channel.send(\"~\", delete_after=0.001)    \n",
        "          \n",
        "        if NoRender == False:\n",
        "          t1 = time.perf_counter()\n",
        "          import cv2\n",
        "          img = cv2.imread('images/custom_img_1.jpg')\n",
        "          height, width, channels = img.shape\n",
        "          ratio = width*height\n",
        "          while height * width > 921600:\n",
        "            img= cv2.resize(img,(0,0), fx=0.95, fy=0.95)\n",
        "            height, width, channels = img.shape\n",
        "          cv2.imwrite('images/custom_img_1.jpg',img)\n",
        "          PATH_TO_TEST_IMAGES_DIR = 'images'\n",
        "          TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'custom_img_1.jpg') ]\n",
        "          IMAGE_SIZE = (12, 8)\n",
        "          async with message.channel.typing():\n",
        "            with detection_graph.as_default():\n",
        "              with tf.Session(graph=detection_graph) as sess1:\n",
        "                image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "                detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "                detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "                detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "                num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "                for image_path in TEST_IMAGE_PATHS:\n",
        "                  image = Image.open(image_path)\n",
        "                  image_np = load_image_into_numpy_array(image)\n",
        "                  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "                  (boxes, scores, classes, num) = sess1.run(\n",
        "                      [detection_boxes, detection_scores, detection_classes, num_detections],\n",
        "                      feed_dict={image_tensor: image_np_expanded})\n",
        "                  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "                      image_np,\n",
        "                      np.squeeze(boxes),\n",
        "                      np.squeeze(classes).astype(np.int32),\n",
        "                      np.squeeze(scores),\n",
        "                      category_index,\n",
        "                      use_normalized_coordinates=True,\n",
        "                      line_thickness=int((height*width)/153600))\n",
        "                  plt.figure(figsize=IMAGE_SIZE)\n",
        "                  #plt.imshow(image_np)\n",
        "                  scipy.misc.imsave('images/result.jpg', image_np)\n",
        "                  t2 = time.perf_counter()\n",
        "                await msg.delete()\n",
        "                file=discord.File(filename=\"result.jpg\", fp=\"images/result.jpg\")\n",
        "                await message.channel.send(content=(\"Completed in: \" + str((round((t2-t1)*1000))) + \"ms\"), file=file)\n",
        "        \n",
        "      elif intent == \"Neural Style\":\n",
        "        embed=discord.Embed(title=\"Neural Style Issues:\", color=0x00a86b)\n",
        "        embed.set_author(name=\"Jade\", url=\"https://discordbots.org/bot/410253782828449802\", icon_url=\"https://preview.ibb.co/jVTDWL/jd-chibi-big.png\")\n",
        "        embed.set_thumbnail(url=\"https://preview.ibb.co/cP9bkf/gif-2.gif\")\n",
        "        issue_count = 1\n",
        "        \n",
        "        t1 = time.perf_counter()\n",
        "      \n",
        "        msg = await message.channel.send(\"...\")\n",
        "        model_id = \"NaN\"\n",
        "        video = False\n",
        "        NoRender=False\n",
        "        file_dl = False\n",
        "      \n",
        "        try:\n",
        "          if \".mp4\" in message.attachments[0].url:\n",
        "            await download_file(message.attachments[0].url,'custom_img_1',\"mp4\")\n",
        "            video = True\n",
        "          else:\n",
        "            await download_file(message.attachments[0].url,'custom_img_1',\"jpg\")\n",
        "          print('\\n' + str(message.guild) + '\\n' + str(message.author) + \": \" + str(message.attachments[0].url))\n",
        "          file_dl=True\n",
        "        except Exception as e:\n",
        "          NoRender = True\n",
        "          embed.add_field(name=\"Issue #\" + str(issue_count), value=\"Cannot download file from link\" \"\\nOr unable to find an attachment\", inline=False)\n",
        "          issue_count = issue_count+1\n",
        "          #await message.channel.send(e)\n",
        "          exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "          fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
        "          print(exc_type, fname, exc_tb.tb_lineno)\n",
        "            \n",
        "        try:\n",
        "          split_str=str(params[\"url\"].values).split('\"')\n",
        "          link = split_str[1]\n",
        "          if \".mp4\" in link:\n",
        "            await download_file(link ,'custom_img_1',\"mp4\")\n",
        "          else:\n",
        "            await download_file(link ,'custom_img_1',\"jpg\")\n",
        "          print('\\n' + str(message.guild) + '\\n' + str(message.author) + \": \" + str(link))\n",
        "          NoRender = False\n",
        "        except Exception as e:\n",
        "          if file_dl==False:\n",
        "            NoRender = True\n",
        "            embed.add_field(name=\"Issue #\" + str(issue_count), value=\"Cannot download file from link\", inline=False)\n",
        "            issue_count = issue_count+1\n",
        "            #await message.channel.send(e)\n",
        "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
        "            print(exc_type, fname, exc_tb.tb_lineno)\n",
        "          \n",
        "        try:\n",
        "          #split_str=str(params[\"type\"].value).split('\"')\n",
        "          #model_id = split_str[1]\n",
        "          model_id = params[\"type\"]\n",
        "          \n",
        "          !ls drive/STYLE_CKPT/models > styles.txt\n",
        "          \n",
        "          file = open(\"styles.txt\",\"r\")\n",
        "          line = file.readlines()\n",
        "          file.close()\n",
        "                    \n",
        "          Accum=\"\"          \n",
        "          for lines in line:\n",
        "            linexd = lines.replace(\".ckpt\", \"\")\n",
        "            Accum = Accum + linexd.replace(\"\\n\",\", \")\n",
        "          \n",
        "          Accum.replace(\".ckpt\",\"\")  \n",
        "            \n",
        "          print(Accum)\n",
        "          print(\"Model: \" + model_id)    \n",
        "              \n",
        "          if model_id not in Accum or model_id == \"\":\n",
        "            print(\"lol\")\n",
        "            NoRender = True\n",
        "            embed.add_field(name=\"Issue #\" + str(issue_count), value='Invalid model type in models: (I found \"' + model_id + '\")\"\\n'+ Accum, inline=False)\n",
        "            issue_count = issue_count+1\n",
        "        except Exception as e:\n",
        "          exc_type, exc_obj, exc_tb = sys.exc_info()\n",
        "          fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
        "          print(exc_type, fname, exc_tb.tb_lineno)\n",
        "          \n",
        "        if video == True:\n",
        "          import cv2\n",
        "          cap = cv2.VideoCapture(\"images/custom_img_1.mp4\")\n",
        "          length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "          if length > 200:\n",
        "            NoRender=True\n",
        "            embed.add_field(name=\"Issue #\" + str(issue_count), value=\"Number of video frames exceeded 200\", inline=False)\n",
        "            issue_count = issue_count+1   \n",
        "            \n",
        "        if NoRender == True:\n",
        "          await message.channel.send(embed=embed, delete_after=15)\n",
        "          async with message.channel.typing():\n",
        "            print('\\n' + str(message.guild) + '\\n' + str(message.author) + \": \" + ModMessage)\n",
        "            ModMessage = ModMessage.replace(\"Jade\", \"Papaya\")\n",
        "            answer = callBot(ModMessage)\n",
        "            answer = answer.replace(\"Papaya\", \"Jade\")\n",
        "            answer = answer.replace(\"father\", \"daughter\")\n",
        "            answer = answer.replace(\"male\", \"female\")\n",
        "            answer = answer.replace(\"boy\", \"girl\")\n",
        "            answer = answer.replace(\"Although being a robot, I look like a normal 9 year old boy.\", \"I look pretty good ;D\")\n",
        "            await msg.edit(content=answer)\n",
        "            print(\"Jade: \" + answer)\n",
        "            await message.channel.send(\"~\", delete_after=0.001)\n",
        "      \n",
        "        t1 = time.perf_counter()\n",
        "        model_id = model_id.lower()\n",
        "        async with message.channel.typing():\n",
        "          if NoRender == False:\n",
        "              \n",
        "            if video == True:\n",
        "              NoRender = True\n",
        "              embed.add_field(name=\"Issue #\" + str(issue_count), value=\"Video still not supported yed :P\", inline=False)\n",
        "              issue_count = issue_count+1 \n",
        "            else:\n",
        "              import cv2\n",
        "              img = cv2.imread('images/custom_img_1.jpg')\n",
        "              height, width, channels = img.shape\n",
        "              ratio = width*height\n",
        "              while height * width > 921600:\n",
        "                img= cv2.resize(img,(0,0), fx=0.95, fy=0.95)\n",
        "                height, width, channels = img.shape\n",
        "              cv2.imwrite('images/custom_img_1.jpg',img)\n",
        "              \n",
        "              !python evaluate.py --in-path images/custom_img_1.jpg --checkpoint drive/STYLE_CKPT/models/{model_id}.ckpt --out-path drive/STYLE_CKPT/result.jpg --device /gpu:0 --allow-different-dimensions\n",
        "              !python evaluate.py --in-path images/custom_img_1.jpg --checkpoint drive/STYLE_CKPT/models/{model_id} --out-path drive/STYLE_CKPT/result.jpg --device /gpu:0 --allow-different-dimensions\n",
        "                            \n",
        "          t2 = time.perf_counter()\n",
        "          \n",
        "        if NoRender == False and not model_id == \"\":\n",
        "          if video == True:\n",
        "            await msg.delete()\n",
        "            file=discord.File(filename=\"result.mp4\", fp=\"drive/STYLE_CKPT/result.mp4\")\n",
        "            await message.channel.send(content=(\"Completed in: \" + str((round((t2-t1)*1000))) + \"ms\"), file=file)\n",
        "          else:\n",
        "            await msg.delete()\n",
        "            file=discord.File(filename=\"result.jpg\", fp=\"drive/STYLE_CKPT/result.jpg\")\n",
        "            await message.channel.send(content=(\"Completed in: \" + str((round((t2-t1)*1000))) + \"ms\"), file=file)\n",
        "        else:\n",
        "          await message.channel.send(\"~\", delete_after=0.001)\n",
        "      \n",
        "      else:\n",
        "        msg = await message.channel.send(\"...\")\n",
        "        async with message.channel.typing():\n",
        "          print('\\n' + str(message.guild) + '\\n' + str(message.author) + \": \" + ModMessage + \" ::: Fallback\")\n",
        "          ModMessage = ModMessage.replace(\"Jade\", \"Papaya\")\n",
        "          answer = callBot(ModMessage)\n",
        "          answer = answer.replace(\"Papaya\", \"Jade\")\n",
        "          answer = answer.replace(\"father\", \"daughter\")\n",
        "          answer = answer.replace(\"male\", \"female\")\n",
        "          answer = answer.replace(\"boy\", \"girl\")\n",
        "          answer = answer.replace(\"Although being a robot, I look like a normal 9 year old boy.\", \"I look pretty good ;D\")\n",
        "          await msg.edit(content=answer)\n",
        "          print(\"Jade: \" + answer)\n",
        "          await message.channel.send(\"~\", delete_after=0.001)\n",
        "          \n",
        "      if message.author.id == 249024790030057472:\n",
        "        await message.channel.send(\"Intent: \" + intent)\n",
        "      print(\"Intent: \" + intent)     \n",
        "\n",
        "client.run(\"TokREEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}